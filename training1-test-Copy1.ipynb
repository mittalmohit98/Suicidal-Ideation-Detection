{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import gensim\n",
    "import ast\n",
    "import pickle\n",
    "from numpy import array, mean\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from gensim import corpora, models\n",
    "#import sklearn.externals.joblib as extjoblib\n",
    "import joblib\n",
    "from sklearn import svm\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "def createTrainingDataset():\n",
    "    GISWDataLength = 1700\n",
    "    GIDataLength = 3000\n",
    "    count = 0\n",
    "    corpus = []\n",
    "    print(\"Extracting training data: \")\n",
    "    with open('cleanedGeneralIssuesCommonTS1.csv', mode='r') as commonReader:\n",
    "        commonReader.readline()\n",
    "        csvGISWReader = csv.reader(commonReader, delimiter=',')\n",
    "        for row in csvGISWReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1:\n",
    "                continue\n",
    "            corpus.append([1, row[1], string])\n",
    "            count += 1\n",
    "            if count > GISWDataLength:\n",
    "                break\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "\n",
    "    count = 0\n",
    "    with open('cleanedGeneralIssuesTS1.csv', mode='r') as generalReader:\n",
    "        generalReader.readline()\n",
    "        csvGIReader = csv.reader(generalReader, delimiter=',')\n",
    "        for row in csvGIReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1 or row[1] == \"[deleted]\":\n",
    "                continue\n",
    "            corpus.append([0, row[1], string])\n",
    "            count += 1\n",
    "            if count > GIDataLength:\n",
    "                break\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    random.shuffle(corpus)\n",
    "    print(\"SHUFFLING->>>>>>>>>>>>>\")\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    with open('training1-copy.csv', mode='w') as writer:\n",
    "        csvWriter = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow(['Class', 'Author', 'Text'])\n",
    "        for row in corpus:\n",
    "            csvWriter.writerow(row)\n",
    "    print(\"Done Extracting\")\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingDataset():\n",
    "    GISWDataLength = 680\n",
    "    GIDataLength = 3000\n",
    "    count = 0\n",
    "    corpus = []\n",
    "    print(\"Extracting testing data: \")\n",
    "    with open('cleanedGeneralIssuesCommonTS2.csv', mode='r') as commonReader:\n",
    "        commonReader.readline()\n",
    "        csvGISWReader = csv.reader(commonReader, delimiter=',')\n",
    "        for row in csvGISWReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1:\n",
    "                continue\n",
    "\n",
    "            count += 1\n",
    "            #if (count < 240):\n",
    "            #    continue\n",
    "            corpus.append([1, row[1], string])\n",
    "            if count > GISWDataLength:\n",
    "                break\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "\n",
    "    count = 0\n",
    "    with open('cleanedGeneralIssuesTS2.csv', mode='r') as generalReader:\n",
    "        generalReader.readline()\n",
    "        csvGIReader = csv.reader(generalReader, delimiter=',')\n",
    "        for row in csvGIReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1 or row[1] == \"[deleted]\":\n",
    "                continue\n",
    "            count += 1\n",
    "            #if (count < 5000):\n",
    "            #   continue\n",
    "            corpus.append([0, row[1], string])\n",
    "            if count > GIDataLength :\n",
    "                break\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    random.shuffle(corpus)\n",
    "    print(\"SHUFFLING->>>>>>>>>>>>>\")\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    with open('testing1.csv', mode='w') as writer:\n",
    "        csvWriter = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow(['Class', 'Author', 'Text'])\n",
    "        for row in corpus:\n",
    "            csvWriter.writerow(row)\n",
    "    print(\"Done Extracting\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModel():\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    with open('training1-copy.csv', mode='r', encoding=\"utf-8\") as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=3:\n",
    "                corpus.append([str(x) for x in row[2].strip().split()])\n",
    "                docNumber += 1\n",
    "\n",
    "    print(\"Total Count: \", docNumber)\n",
    "    dictionary = gensim.corpora.Dictionary(corpus)\n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "    print(len(dictionary))\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=450)\n",
    "    print(len(dictionary))\n",
    "    #count = 0\n",
    "    #for k, v in dictionary.iteritems():\n",
    "     #   print(k, v)\n",
    "      #  count += 1\n",
    "       # if count > 10:\n",
    "        #    break\n",
    "    #print(\"corpus:\")\n",
    "    #print(corpus[1:100])\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    bow_corpus[1500]\n",
    "    #print(\"bow_corpus:\")\n",
    "    #print(bow_corpus[1:100])\n",
    "    \n",
    "    bow_doc_4310=bow_corpus[1500]\n",
    "    for i in range(len(bow_doc_4310)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],\n",
    "                                                         dictionary[bow_doc_4310[i][0]],\n",
    "                                                         bow_doc_4310[i][1]))\n",
    "    \n",
    "    \"\"\"tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    from pprint import pprint\n",
    "    for doc in corpus_tfidf:\n",
    "        pprint(doc)\n",
    "        break\n",
    "    \"\"\"\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=2, id2word=dictionary, passes=10, workers=4)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "        \n",
    "        \n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    with open('testing1-copy.csv', mode='r', encoding=\"utf-8\") as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=3:\n",
    "                corpus.append([str(x) for x in row[2].strip().split()])\n",
    "                docNumber += 1\n",
    "\n",
    "    print(\"Total Count: \", docNumber)\n",
    "    dictionary = gensim.corpora.Dictionary(corpus)\n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "    print(len(dictionary))\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=450)\n",
    "    print(len(dictionary))\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    #bow_corpus[5912]\n",
    "    #print(\"bow_corpus:\")\n",
    "    #print(bow_corpus[1:100])\n",
    "    \n",
    "    bow_doc_4310=bow_corpus[770]\n",
    "    for i in range(len(bow_doc_4310)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],\n",
    "                                                         dictionary[bow_doc_4310[i][0]],\n",
    "                                                         bow_doc_4310[i][1]))\n",
    "    \n",
    "   \n",
    "    print(lda_model.get_document_topics(bow_doc_4310)) \n",
    "    \"\"\"    \n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=1, workers=4)\n",
    "    \n",
    "    for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "        print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    corpus[5912]\n",
    "    for index, score in sorted(lda_model[bow_corpus[5912]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    \n",
    "    for index, score in sorted(lda_model_tfidf[bow_corpus[5912]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "    # save the model to disk\n",
    "    #modelFile = datapath('/Users/Dell/final major project/ldaTrainedModel')\n",
    "    #lda_model.save(modelFile)\n",
    "    #filename = 'finalized_model.sav'\n",
    "    #pickle.dump(model, open(filename, 'wb'))\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestTopicModel():\n",
    "    searchParams = {'n_components': [n for n in range(2, 7)], 'learning_decay': [0.3,0.5,0.7,0.9]}\n",
    "    lda = LatentDirichletAllocation()\n",
    "    model = GridSearchCV(lda, searchParams)\n",
    "\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    with open('training1-copy.csv', mode='r') as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                if (int(row[0]) == 1):\n",
    "                    corpus.append(\" \".join(ast.literal_eval(row[2])))\n",
    "                    docNumber += 1\n",
    "    print(\"Total Doc: \", docNumber)\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=10)\n",
    "    data_vectorized = vectorizer.fit_transform(corpus)\n",
    "    print('Started Training')\n",
    "    model.fit(data_vectorized)\n",
    "    print('Found the best model')\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "    # Log Likelihood Score\n",
    "    print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "    # Perplexity\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "    filename = 'finalized_LDAModel.sav'\n",
    "    pickle.dump(best_lda_model, open(filename, 'wb'))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainFeatures(filename):\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    suicidalCount = 0\n",
    "    nonsuicidalCount = 0\n",
    "    with open(filename, mode='r') as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                corpus.append(array([row[0], \" \".join(ast.literal_eval(row[2]))]))\n",
    "                if (row[0] == '1'):\n",
    "                    suicidalCount += 1\n",
    "                else:\n",
    "                    nonsuicidalCount += 1\n",
    "                docNumber += 1\n",
    "    print(\"Total Doc: \", docNumber)\n",
    "    print(\"Suicidal Count: \", suicidalCount)\n",
    "    print(\"Non-Suicidal Count: \", nonsuicidalCount)\n",
    "    corpus = array(corpus)\n",
    "    print(corpus[:,1])\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=10)\n",
    "    data_vectorized = vectorizer.fit_transform(corpus[:, 1])\n",
    "    print(data_vectorized.shape)\n",
    "    print(vectorizer.vocabulary_.get('depress'))\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(data_vectorized)\n",
    "    X_train_tf = tf_transformer.transform(data_vectorized)\n",
    "    print(X_train_tf.shape)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(data_vectorized)\n",
    "    print(X_train_tfidf.shape)\n",
    "\n",
    "    return X_train_tfidf, corpus[:, 0], vectorizer, tfidf_transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestFeatures(filename):\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    suicidalCount = 0\n",
    "    nonsuicidalCount = 0\n",
    "    with open(filename, mode='r') as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                corpus.append(array([row[0], \" \".join(ast.literal_eval(row[2]))]))\n",
    "                if(row[0] == '1'):\n",
    "                    suicidalCount += 1\n",
    "                else:\n",
    "                    nonsuicidalCount += 1\n",
    "                docNumber += 1\n",
    "    print(\"Total Doc: \", docNumber)\n",
    "    print(\"Suicidal Count: \", suicidalCount)\n",
    "    print(\"Non-Suicidal Count: \", nonsuicidalCount)\n",
    "    corpus = array(corpus)\n",
    "    return corpus\n",
    "    # print(corpus[:,1])\n",
    "\n",
    "    # vectorizer = CountVectorizer(min_df=10)\n",
    "    # data_vectorized = vectorizer.transform(corpus[:, 1])\n",
    "    # print(data_vectorized.shape)\n",
    "    # print(vectorizer.vocabulary_.get('depress'))\n",
    "    #\n",
    "    # tf_transformer = TfidfTransformer(use_idf=False).fit(data_vectorized)\n",
    "    # X_train_tf = tf_transformer.transform(data_vectorized)\n",
    "    # print(X_train_tf.shape)\n",
    "    #\n",
    "    # tfidf_transformer = TfidfTransformer()\n",
    "    # X_train_tfidf = tfidf_transformer.fit_transform(data_vectorized)\n",
    "    # print(X_train_tfidf.shape)\n",
    "\n",
    "    # return X_train_tfidf, corpus[:, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM():\n",
    "    X_train_tfidf, target, countVectorizer, tfidf_transformer = getTrainFeatures('training1-copy.csv')\n",
    "    print('Started Training...')\n",
    "    #clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=500, tol=None)\n",
    "    #clf = MultinomialNB()\n",
    "    #clf = svm.SVC(kernel='rbf')\n",
    "    clf = RandomForestClassifier(max_depth = 6, random_state=0)\n",
    "    clf.fit(X_train_tfidf, target)\n",
    "    print('Done Training...')\n",
    "    print('Started Testing...')\n",
    "\n",
    "    testingCorpus = getTestFeatures('testing1.csv')\n",
    "    X_test_counts = countVectorizer.transform(testingCorpus[:, 1])\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "    print(len(target))\n",
    "    predictions = clf.predict(X_test_tfidf)\n",
    "    print(predictions)\n",
    "    print(target)\n",
    "    print(mean(predictions == testingCorpus[:, 0]))\n",
    "    print(confusion_matrix(testingCorpus[:, 0], predictions))\n",
    "    print(confusion_matrix(testingCorpus[:, 0], predictions).ravel())\n",
    "\n",
    "    \"\"\"validatingCorpus = getTestFeatures('validation1.csv')\n",
    "    X_test_counts = countVectorizer.transform(validatingCorpus[:, 1])\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "    # print(len(test_target))\n",
    "    # predictions = clf.predict(X_test_tfidf)\n",
    "    print(predictions)\n",
    "    # print(test_target)\n",
    "    print(mean(predictions == validatingCorpus[:, 0]))\n",
    "    print(confusion_matrix(validatingCorpus[:, 0], predictions))\n",
    "    print(confusion_matrix(validatingCorpus[:, 0], predictions).ravel())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training data: \n",
      "[1, 'Plassholdertekst', ['feel', 'like', 'actual', 'feel', 'normal', 'happi', 'fall', 'hard', 'happi', 'alway', 'part', 'cycl', 'depress']]\n",
      "[1, 'Limnir', ['constantli', 'wish', 'peopl', 'dead', 'cancer', 'recent', 'older', 'stepbroth', 'live', 'sick', 'constantli', 'tell', 'younger', 'brother', 'wish', 'dead', 'never', 'second', 'neither', 'say', 'like', 'around', 'time', 'sick', 'wish', 'peopl', 'dead', 'time', 'say', 'face', 'someth', 'like', 'work', 'one', 'thing', 'time', 'everyon', 'els', 'littl', 'brother', 'time', 'say', 'turn', 'peopl', 'anyth', 'littl', 'fix', 'broken', 'time', 'everyth', 'tell', 'wish', 'kill', 'older', 'stepbroth', 'say', 'pay', 'hous', 'noth', 'brother', 'think', 'sick', 'think', 'normal', 'wish', 'littl', 'brother', 'dead']]\n",
      "[1, 'Plassholdertekst', ['often', 'final', 'feel', 'realli', 'happi', 'good', 'suddenli', 'realli', 'bad', 'could', 'mean', 'bipolar', 'type', 'happi', 'real']]\n",
      "[1, 'SHThrowAway213', ['let', 'know', 'said', 'girl', 'said', 'keep', 'eye', 'still', 'realli', 'coupl', 'sinc', 'told', 'panic', 'alreadi', 'get', 'intens', 'help', 'want', 'anyon', 'els', 'gone', 'hell', 'way', 'keep', 'anyon', 'els', 'safe', 'polic', 'help', 'reluct', 'fault', 'anyon', 'els']]\n",
      "[1, 'SHThrowAway213', ['went', 'well', 'met', 'around', 'hour', 'coffe', 'got', 'realli', 'well', 'great', 'convers', 'live', 'town', 'cute', 'realli', 'nice', 'funni', 'gorgeou', 'realli', 'nice', 'person', 'quit', 'shi', 'first', 'tri', 'shi', 'said', 'could', 'tell', 'meet', 'nice', 'walk', 'good', 'go', 'scare', 'away', 'mental', 'health', 'chang', 'good', 'bad', 'know', 'tell', 'hope', 'scare']]\n",
      "[0, 'lurpelis', ['escap', 'life', 'pointless']]\n",
      "[0, 'phantomnostalgia', ['tire', 'search', 'solut', 'toler', 'everyon', 'independ', 'ego', 'want', 'show', 'good', 'tri', 'best', 'everyday', 'show', 'compass', 'love', 'fact', 'made', 'quit', 'horribl', 'time', 'relaps', 'turn', 'bitter', 'last', 'year', 'high', 'school', 'everyon', 'enter', 'univers', 'mental', 'disabl', 'studi', 'actual', 'could', 'life', 'move', 'lost', 'best', 'friend', 'due', 'mad', 'meet', 'person', 'made', 'feel', 'special', 'anxieti', 'took', 'opportun', 'away', 'could', 'contact', 'frankli', 'worth', 'time', 'made', 'new', 'friend', 'also', 'away', 'anoth', 'town', 'everyon', 'connect', 'go', 'away', 'fault', 'life', 'everyth', 'motion', 'stand', 'wish', 'could', 'jump', 'window', 'make', 'brain']]\n",
      "[0, 'duskdawn95', ['throughout', 'life', 'alway', 'sort', 'time', 'realli', 'lot', 'troubl', 'make', 'get', 'anxieti', 'talk', 'new', 'peopl', 'around', 'lot', 'peopl', 'realli', 'made', 'depress', 'colleg', 'realli', 'wish', 'better', 'person', 'realli', 'emot', 'intens', 'guy', 'realli', 'taken', 'toll', 'well', 'anyth', 'anyon', 'crush', 'realli', 'need', 'stop', 'well', 'realli', 'want', 'stop', 'roller', 'coaster', 'flow', 'life', 'descript', 'anti', 'depress', 'take', 'worri', 'weight', 'gain', 'sinc', 'skinni', 'sleepi', 'would', 'problem', 'ask', 'anyth', 'need', 'answer', 'question', 'thank']]\n",
      "[0, 'Ramenhearts', ['coupl', 'sort', 'sad', 'call', 'depress', 'sure', 'depress', 'moodi', 'day', 'feel', 'bright', 'day', 'like', 'never', 'end', 'head', 'never', 'suicid', 'consid', 'make', 'suicid', 'note', 'let', 'note', 'support', 'suicid', 'respect', 'take', 'away', 'also', 'believ', 'alway', 'better', 'solut', 'problem', 'one', 'suicid', 'note', 'ask', 'instead', 'note', 'someon', 'right', 'someon', 'suicid', 'messag', 'direct', 'whoever', 'find', 'either', 'get', 'choos', 'commit', 'suicid', 'note', 'mostli', 'introvert', 'person', 'realli', 'hard', 'time', 'commun', 'peopl', 'usual', 'talk', 'mani', 'besid', 'group', 'best', 'type', 'person', 'everyon', 'get', 'know', 'got', 'upset', 'first', 'place', 'note', 'plan', 'achiev', 'tell', 'stori', 'whoever', 'would', 'make', 'obviou', 'written', 'specif', 'suicid', 'scrape', 'previou', 'difficult', 'explain', 'perhap', 'sometim', 'futur', 'may', 'may', 'post', 'exampl', 'thing', 'found', 'commit', 'write', 'fulli', 'honest', 'note', 'like', 'sad', 'realli', 'perman', 'found', 'self', 'mood', 'pour', 'onto', 'paper', 'stop', 'mood', 'becom', 'believ', 'need', 'write', 'note', 'believ', 'order', 'write', 'behind', 'feel', 'like', 'need', 'sad', 'throughout', 'whole', 'write', 'piec', 'properli', 'thank', 'read', 'shown', 'peopl', 'compass', 'never', 'alon', 'world', 'hope', 'fight', 'depress', 'right', 'one', 'day', 'find', 'day']]\n",
      "[0, 'jjwf3', ['slowli', 'watch', 'peopl', 'could', 'call', 'ran', 'away', 'crazi', 'accomplish', 'realli', 'fail', 'everyth', 'tri']]\n",
      "SHUFFLING->>>>>>>>>>>>>\n",
      "[0, 'strobeliteshurricane', ['nobodi', 'left', 'someon', 'els', 'worst', 'part', 'suffer', 'self', 'day', 'feel', 'time', 'feel', 'empti', 'feel', 'stomach', 'feel', 'physic', 'sensat', 'hopeless', 'brain', 'battl', 'want', 'die', 'hand', 'know', 'noth', 'go', 'ever', 'chang', 'tri', 'act', 'impuls', 'everyth', 'plain', 'hard', 'feel', 'rage', 'act', 'public', 'hard', 'break', 'cri', 'walk', 'tri', 'show', 'normal', 'fall', 'love', 'someon', 'hurt', 'replac', 'willingli', 'chose', 'block', 'commun', 'person', 'talk', 'someon', 'els', 'leav', 'broke', 'suicid', 'knew', 'like', 'said', 'want', 'die', 'choic', 'point']]\n",
      "[0, 'tuddaj', ['hello', 'look', 'book', 'read', 'one', 'self', 'help', 'get', 'depress', 'one', 'enough', 'get', 'mind', 'neg', 'depress', 'make', 'sens', 'thank']]\n",
      "[0, 'myeyesshinelikestars', ['bad', 'immatur', 'old', 'back', 'huge', 'free', 'spirit', 'know', 'immatur', 'age', 'howev', 'sinc', '2012', 'come', 'back', 'bite', 'as', 'two', 'side', 'know', 'hard', 'young', 'stupid', 'time', 'also', 'feel', 'like', 'deserv', 'feel', 'like', 'anyth', 'illeg', 'know', 'hurt', 'lot', 'peopl', 'two', 'year', 'period', 'old', 'shell', 'former', 'teen', 'self', 'big', 'heart', 'love', 'gentl', 'would', 'anyth', 'anyon', 'rude', 'immatur', 'selfish', 'person', 'want', 'eventu', 'help', 'peopl', 'especi', 'want', 'apolog', 'everyon', 'hurt', 'also', 'want', 'open', 'old', 'wound', 'know', 'want', 'feel', 'better', 'move', 'life', 'time', 'think', 'happi', 'move', 'life', 'would', 'thank']]\n",
      "[0, 'TaphGiraffe', ['feel', 'like', 'entir', 'life', 'wast', 'struggl', 'depress', 'anxieti', 'moment', 'last', 'summer', 'school', 'sort', 'last', 'hope', 'thing', 'individu', 'group', 'nice', 'actual', 'talk', 'peopl', 'feel', 'like', 'realli', 'help', 'super', 'depress', 'late', 'time', 'arriv', 'spring', 'weather', 'feel', 'like', 'idiot', 'walk', 'around', 'campu', 'jacket', 'self', 'consciou', 'skinni', 'bodi', 'awkward', 'stage', 'grow', 'hair', 'gotten', 'use', 'smile', 'neutral', 'face', 'hide', 'depress', 'peopl', 'realli', 'tri', 'hard', 'close', 'anyon', 'tri', 'distanc', 'famili', 'come', 'recent', 'liter', 'one', 'talk', 'old', 'poor', 'jobless', 'car', 'le', 'know', 'much', 'live', 'real', 'world', 'honestli', 'come', 'strict', 'forc', 'child', 'know', 'life', 'live', 'mysteri', 'feel', 'like', 'depress', 'end', 'get', 'suspend', 'colleg', 'back', 'last', 'time', 'univers', 'citi', 'home', 'spent', 'last', 'week', 'spring', 'alon', 'dorm', 'room', 'start', 'doubt', 'choic', 'major', 'comput', 'engin', 'take', 'final', 'decid', 'one', 'fall', 'behind', 'schoolwork', 'life', 'wast', 'day', 'away', 'watch', 'anim', 'listen', 'music', 'loos', 'pa', 'dread', 'wake', 'everi', 'morn', 'know', 'fix', 'life', 'feel', 'like', 'noth', 'school', 'last', 'feel', 'like', 'space', 'take', 'everi', 'breath', 'breath', 'food', 'forc', 'eat', 'put', 'better', 'use', 'someon', 'actual', 'passion', 'live']]\n",
      "[0, 'Snow_Viking', ['well', 'nice', 'support', 'someon', 'even', 'alcohol', 'nicotin']]\n",
      "[0, 'be-blissful', ['liter', 'final', 'good', 'day', 'think', 'good', 'huge', 'wave', 'depress', 'come', 'two', 'forward', 'million', 'back']]\n",
      "[0, 'gentlegiantlover', ['necessarili', 'talk', 'depress', 'relief', 'anyth', 'know', 'system', 'side', 'effect', 'anyth', 'know', 'got', 'brain', 'rel', 'quickli', 'tri', 'prepar', 'effect', 'lack', 'anyth', 'gener', 'use']]\n",
      "[0, 'ohshitfinances', ['lost', 'job', 'end', 'last', 'year', 'insur', 'run', 'low', 'afford', 'pocket', 'doctor', 'chang', 'came', 'day', 'plan', 'pill', 'left', 'abl', 'get', 'though', 'today', 'straight', 'full', 'dose', 'taper', 'afford', 'privat', 'insur', 'pocket', 'cost', 'plan', 'could', 'find', 'even', 'met', '200', 'cover', 'offic', 'might', 'well', 'million', 'right', 'wait', 'sinc', 'approv', 'suppos', 'take', 'day', 'check', 'last', 'week', 'said', 'back', 'could', 'take', 'day', 'hear', 'back', 'went', 'local', 'free', 'clinic', 'told', 'wait', 'time', 'see', 'gener', 'doctor', 'long', 'psychiatrist', 'guarante', 'actual', 'see', 'one', 'today', 'sweat', 'stomach', 'upset', 'feel', 'nauseou', 'worri', 'go', 'bad', 'withdraw', 'brain', 'told', 'anoth', 'person', 'way', 'get', 'help', 'say', 'suicid', 'even', 'accord', 'stori', 'terrifi', 'experi', 'much', 'want', 'rather', 'die', 'way', 'abl', 'without', 'insur', 'feel', 'hopeless', 'sick', 'safeti', 'net', 'stuck', 'without', 'help', 'actual', 'make', 'feel', 'would', 'easier', 'gave', 'bad', 'withdraw', 'realli', 'effect', 'would', 'like', 'therapi', 'anyth', 'anyon', 'abl', 'take', 'medic', 'transit', 'withdraw']]\n",
      "[1, 'redpillthrowaway20', ['fat', 'ugli', 'look', 'guy', 'tri', 'date', 'even', 'work', 'week', 'one', 'singl', 'around', 'age', 'age']]\n",
      "[0, 'skylukewalker99', ['last', 'sever', 'came', 'new', 'low', 'today', 'let', 'start', 'way', 'back', '2013', 'back', 'friend', 'let', 'call', 'met', 'class', 'activ', 'commun', 'one', 'stupid', 'relat', 'coupl', 'ran', 'one', 'live', 'state', 'halfway', 'across', 'countri', 'let', 'call', 'talk', 'friendli', 'time', 'would', 'play', 'togeth', 'everi', 'night', 'one', 'night', 'best', 'friend', 'log', 'call', 'show', 'face', 'hilari', 'roll', 'one', 'peopl', 'ever', 'met', 'later', 'night', 'friend', 'look', 'stalk', 'profil', 'second', 'saw', 'pictur', 'beauti', 'continu', 'talk', 'sever', 'first', 'talk', '2013', 'overal', 'grew', 'close', 'everi', 'day', 'one', 'day', '2014', 'realli', 'nervou', 'huge', 'crush', 'time', 'dislik', 'toward', 'even', 'state', 'intim', 'sever', 'crush', 'eventu', 'got', 'straight', 'ask', 'said', 'well', 'yeah', 'dont', 'want', 'ruin', 'said', 'reason', 'like', 'current', 'felt', 'like', 'date', 'instead', 'offici', 'coupl', '2014', 'may', 'save', 'enough', 'money', 'see', 'weekend', 'one', 'best', 'life', 'never', 'felt', 'much', 'passion', 'love', 'person', 'abl', 'see', 'everi', 'hard', 'worth', 'much', 'teenag', 'boy', 'teenag', 'stupid', 'full', 'august', 'went', 'concert', 'one', 'best', 'girl', 'made', 'told', 'immedi', 'fit', 'guilt', 'right', 'told', 'stay', 'probabl', 'close', 'trust', 'awar', 'piec', 'excus', 'tri', 'say', 'worri', 'wors', 'grew', 'back', 'togeth', 'month', 'stop', 'talk', 'got', 'lone', 'stupid', 'got', 'hot', 'august', 'met', 'girl', 'bit', 'meet', 'never', 'found', 'close', 'said', 'done', 'reason', 'stop', 'talk', 'never', 'gave', 'space', 'peopl', 'hurt', 'still', 'said', 'kept', 'peopl', 'one', 'girl', 'say', 'realli', 'sex', 'said', 'tell', 'right', 'furiou', 'believ', 'still', 'thought', 'abl', 'fix', 'ruin', 'everi', 'chanc', 'love', 'would', 'come', 'back', 'today', 'told', 'love', 'never', 'felt', 'empti', 'alon', 'one', 'tell', 'feel', 'like', 'light', 'insid', 'gone', 'think', 'singl', 'posit', 'thought', 'sinc', 'know', 'post', 'guess', 'weak', 'hope', 'mayb', 'someon', 'tell', 'reason', 'believ', 'know']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Extracting\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    createTrainingDataset()\n",
    "    #createTestingDataset()\n",
    "    #createValidationDataset()\n",
    "    #trainingModel()\n",
    "    #bestTopicModel()\n",
    "    # SVM()\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count:  3352\n",
      "0 'caus',\n",
      "1 'cover',\n",
      "2 'everyth']\n",
      "3 'get',\n",
      "4 'go',\n",
      "5 'help',\n",
      "6 'hospit',\n",
      "7 'insur',\n",
      "8 'know',\n",
      "9 'never',\n",
      "10 'pay',\n",
      "5697\n",
      "450\n",
      "Word 1 (\"'get',\") appears 1 time.\n",
      "Topic: 0 \n",
      "Words: 0.018*\"'time',\" + 0.015*\"'know',\" + 0.014*\"'one',\" + 0.013*\"'even',\" + 0.013*\"'would',\" + 0.013*\"'life',\" + 0.013*\"'realli',\" + 0.012*\"'tri',\" + 0.012*\"'peopl',\" + 0.009*\"'think',\"\n",
      "Topic: 1 \n",
      "Words: 0.028*\"'get',\" + 0.025*\"'want',\" + 0.018*\"'know',\" + 0.016*\"'depress',\" + 0.014*\"'peopl',\" + 0.013*\"'time',\" + 0.013*\"'think',\" + 0.012*\"'life',\" + 0.011*\"'day',\" + 0.011*\"'work',\"\n",
      "Total Count:  4108\n",
      "0 'dose',\n",
      "1 'littl']\n",
      "2 'look',\n",
      "3 'much',\n",
      "4 'see',\n",
      "5 'time',\n",
      "6 ['first',\n",
      "7 'affect',\n",
      "8 'almost',\n",
      "9 'alon',\n",
      "10 'along',\n",
      "5920\n",
      "450\n",
      "Word 0 (\"'look',\") appears 1 time.\n",
      "Word 1 (\"'much',\") appears 3 time.\n",
      "Word 2 (\"'see',\") appears 2 time.\n",
      "Word 3 (\"'time',\") appears 3 time.\n",
      "Word 4 (\"'almost',\") appears 1 time.\n",
      "Word 5 (\"'alon',\") appears 1 time.\n",
      "Word 9 (\"'basic',\") appears 1 time.\n",
      "Word 10 (\"'broke',\") appears 1 time.\n",
      "Word 15 (\"'depress',\") appears 1 time.\n",
      "Word 16 (\"'due',\") appears 1 time.\n",
      "Word 18 (\"'ever',\") appears 2 time.\n",
      "Word 19 (\"'famili',\") appears 3 time.\n",
      "Word 20 (\"'get',\") appears 3 time.\n",
      "Word 22 (\"'help',\") appears 1 time.\n",
      "Word 23 (\"'ill',\") appears 2 time.\n",
      "Word 26 (\"'life',\") appears 2 time.\n",
      "Word 28 (\"'love',\") appears 1 time.\n",
      "Word 29 (\"'mental',\") appears 2 time.\n",
      "Word 31 (\"'money',\") appears 1 time.\n",
      "Word 32 (\"'move',\") appears 1 time.\n",
      "Word 33 (\"'need',\") appears 1 time.\n",
      "Word 34 (\"'never',\") appears 2 time.\n",
      "Word 35 (\"'old',\") appears 1 time.\n",
      "Word 42 (\"'sinc',\") appears 2 time.\n",
      "Word 46 (\"'talk',\") appears 1 time.\n",
      "Word 48 (\"'two',\") appears 1 time.\n",
      "Word 49 (\"'want',\") appears 2 time.\n",
      "Word 51 (\"'wors',\") appears 1 time.\n",
      "Word 57 (\"'deal',\") appears 1 time.\n",
      "Word 65 (\"'problem',\") appears 1 time.\n",
      "Word 70 (\"'suicid',\") appears 2 time.\n",
      "Word 72 (\"'way',\") appears 3 time.\n",
      "Word 74 (\"'even',\") appears 2 time.\n",
      "Word 78 (\"'tri',\") appears 2 time.\n",
      "Word 80 (\"'write',\") appears 2 time.\n",
      "Word 81 (\"'absolut',\") appears 1 time.\n",
      "Word 91 (\"'would',\") appears 1 time.\n",
      "Word 92 (\"'afraid',\") appears 2 time.\n",
      "Word 93 (\"'away',\") appears 1 time.\n",
      "Word 96 (\"'could',\") appears 1 time.\n",
      "Word 97 (\"'cri',\") appears 1 time.\n",
      "Word 101 (\"'give',\") appears 1 time.\n",
      "Word 103 (\"'got',\") appears 2 time.\n",
      "Word 104 (\"'hard',\") appears 1 time.\n",
      "Word 106 (\"'job',\") appears 1 time.\n",
      "Word 109 (\"'mayb',\") appears 1 time.\n",
      "Word 116 (\"'actual',\") appears 1 time.\n",
      "Word 119 (\"'anyon',\") appears 1 time.\n",
      "Word 123 (\"'everi',\") appears 1 time.\n",
      "Word 124 (\"'fact',\") appears 1 time.\n",
      "Word 126 (\"'know',\") appears 9 time.\n",
      "Word 129 (\"'quit',\") appears 2 time.\n",
      "Word 130 (\"'real',\") appears 1 time.\n",
      "Word 131 (\"'say',\") appears 2 time.\n",
      "Word 133 (\"'stop',\") appears 2 time.\n",
      "Word 136 (\"'accept',\") appears 1 time.\n",
      "Word 142 (\"'handl',\") appears 1 time.\n",
      "Word 146 (\"'one',\") appears 4 time.\n",
      "Word 147 (\"'past',\") appears 1 time.\n",
      "Word 150 (\"'still',\") appears 1 time.\n",
      "Word 153 (\"'today',\") appears 1 time.\n",
      "Word 156 (\"'everyon',\") appears 1 time.\n",
      "Word 157 (\"'everyth',\") appears 1 time.\n",
      "Word 168 (\"'home',\") appears 1 time.\n",
      "Word 173 (\"'mani',\") appears 1 time.\n",
      "Word 175 (\"'person',\") appears 1 time.\n",
      "Word 179 (\"'someon',\") appears 1 time.\n",
      "Word 185 (\"'watch',\") appears 2 time.\n",
      "Word 186 (\"'alway',\") appears 1 time.\n",
      "Word 190 (\"'possibl',\") appears 1 time.\n",
      "Word 192 (\"'use',\") appears 1 time.\n",
      "Word 193 (\"'well',\") appears 1 time.\n",
      "Word 196 (\"'day',\") appears 1 time.\n",
      "Word 197 (\"'night',\") appears 1 time.\n",
      "Word 200 (\"'thing',\") appears 2 time.\n",
      "Word 210 (\"'gener',\") appears 1 time.\n",
      "Word 213 (\"'read',\") appears 1 time.\n",
      "Word 217 (\"'first',\") appears 2 time.\n",
      "Word 218 (\"'happi',\") appears 1 time.\n",
      "Word 220 (\"'right',\") appears 1 time.\n",
      "Word 237 (\"'fast',\") appears 1 time.\n",
      "Word 238 (\"'felt',\") appears 1 time.\n",
      "Word 245 (\"'liter',\") appears 1 time.\n",
      "Word 247 (\"'met',\") appears 1 time.\n",
      "Word 250 (\"'three',\") appears 1 time.\n",
      "Word 275 (\"'year',\") appears 2 time.\n",
      "Word 280 (\"'come',\") appears 1 time.\n",
      "Word 288 (\"'last',\") appears 1 time.\n",
      "Word 289 (\"'later',\") appears 1 time.\n",
      "Word 299 (\"'thought',\") appears 1 time.\n",
      "Word 301 (\"'went',\") appears 1 time.\n",
      "Word 307 (\"'may',\") appears 1 time.\n",
      "Word 311 (\"'said',\") appears 3 time.\n",
      "Word 314 (\"'differ',\") appears 1 time.\n",
      "Word 315 (\"'drink',\") appears 4 time.\n",
      "Word 320 (\"'play',\") appears 1 time.\n",
      "Word 324 (\"'whole',\") appears 1 time.\n",
      "Word 334 (\"'guess',\") appears 1 time.\n",
      "Word 337 (\"'kind',\") appears 1 time.\n",
      "Word 343 (\"'abus',\") appears 1 time.\n",
      "Word 344 (\"'apart',\") appears 1 time.\n",
      "Word 345 (\"'came',\") appears 1 time.\n",
      "Word 352 (\"'father',\") appears 1 time.\n",
      "Word 353 (\"'found',\") appears 2 time.\n",
      "Word 356 (\"'knew',\") appears 2 time.\n",
      "Word 360 (\"'mother',\") appears 3 time.\n",
      "Word 361 (\"'pretti',\") appears 1 time.\n",
      "Word 362 (\"'rememb',\") appears 1 time.\n",
      "Word 365 (\"'sister',\") appears 4 time.\n",
      "Word 369 (\"'told',\") appears 2 time.\n",
      "Word 370 (\"'took',\") appears 1 time.\n",
      "Word 391 (\"'half',\") appears 1 time.\n",
      "Word 396 (\"'miss',\") appears 1 time.\n",
      "Word 423 (\"'room',\") appears 1 time.\n",
      "Word 437 (\"'pay',\") appears 1 time.\n",
      "Word 440 (\"'seen',\") appears 1 time.\n",
      "[(0, 0.5574666), (1, 0.44253334)]\n"
     ]
    }
   ],
   "source": [
    "trainingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  997\n",
      "two usual depress therapi quit expens though wonder anyon provid good insight like know therapist like gener idea also use impact peopl wonder treatment right thank\n",
      "assum ask hate probabl known mani\n",
      "well put feel given life without choic never truli meant born world drag age young even think sort way life preciou give still young true genuin mental emot somewhat physic disabl old mother left anyon lean ye step father half sister mention rest famili give support take much intern pain given govern money known social secur disabl incom young naiv sat told realli cours abl help social secur took told lack date back sent profession help group peopl similar sent mani time age old old time due sent place rest alway suicid patient still way younger major depress high anxieti panic autist spectrum disord function limb weak suicid black cognit suffer cancer babi gone chemotherapi make huge toll affect life drastic graduat high school diploma certif complet get colleg tri go ged score averag took 200 money well social secur countless pay 000 even struggl tri find job pay done gotten interview major depress tri file appeal back learn back want abl support properli think make waiver money declin waiver yet think work full time day week job suffer lot mayb day week work day know disabl even find job young person deal realli especi mental hard young peopl find job even harder peopl like get job left noth truli want life end feel worthless dead beat societi helpless nobodi someon valu life wast life blood mockeri human race deal struggl much money come feel take life suffer better end life easier live life everyday pain miser past eat littl dinner depress head dark neg lost appetit due wast away want noth life end feel peac mother one truli known gone without tri kill countless time younger get 100 success rate due fail dad stop suicid self harm done burn skin overdos pain tri tri drink cut wrist swallow foreign tri jump want die\n",
      "dont know miss day work time drink think wors least time better live citi almost weekend without talk singl person reason interact peopl week job satisfact enjoy feel empti insid imposs fill feel broken unfix dark see light sometim think want loneli might biggest culprit unabl meaning friendship current state self bare look mirror mani unabl commun reject never chanc accept make logic know better cant bring realli know post guess thought would good least attempt reach make rash decis\n",
      "hey get good respons thought tri dont realli know talk alway felt better talk peopl feel like peopl know theyr sick tire littl talk sick tire everyth dont actual know that though guess ill tell pretti bad good one theyr think someth depart make degre worthless even though first year five year degre first semest cant seem make like one bad one talk good peopl talk ration dont much doesnt realli work quiet let talk want talk cant think stuff say that sad depress sure ever saw told feel guilti talk dont want either tell love one well love love one romant amaz great amaz person come mind think someon spend rest life that gotten point obsess rememb noth actual feel right girl practic sens know amaz wish love friend love le fact lot dont particularli like love lot like togeth togeth still wish though there also lie guess made believ thought marri believ break mess wish could help inabl commit someon need take care lot lost feel cant much without still though time time strong girl delusion get togeth deal good person there also univers yeah guess wish could leav graduat back job back famili everyth tie dont want think famili two separ left famili lot way good bad like everyth theyr absolut theyr absolut bad absolut good provid beyond need practic princip caviti heart forc via give mani like didnt matter much ye tri talk understand human feel love act way wish could think outsid box career job money would rather clean food shelter happi rich cold clinic manner suggest worst part honest good think help give realiz gove fit 1800 old fashion old peopl old cultur cant realli blame guess that pretti much tri kill thank appar drug tri overdos cant kill wont post name someon make mistak yeah start le le thank die day wish could know sort happi could relationship like wish could attend univers abil studi could someth better vocat job option due class countri famili yeah that pretti much day think smoke cigarett go increas rate wish could tell two dont talk around friendship want last turn artifici think good theyr good peopl sorri took long\n",
      "Started Training\n",
      "Found the best model\n",
      "Best Model's Params:  {'learning_decay': 0.7, 'n_components': 2}\n",
      "Best Log Likelihood Score:  -77766.76897552879\n",
      "Model Perplexity:  478.1329118477133\n"
     ]
    }
   ],
   "source": [
    "bestTopicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  4552\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  3001\n",
      "['nobodi left someon els worst part suffer self day feel time feel empti feel stomach feel physic sensat hopeless brain battl want die hand know noth go ever chang tri act impuls everyth plain hard feel rage act public hard break cri walk tri show normal fall love someon hurt replac willingli chose block commun person talk someon els leav broke suicid knew like said want die choic point'\n",
      " 'hello look book read one self help get depress one enough get mind neg depress make sens thank'\n",
      " 'bad immatur old back huge free spirit know immatur age howev sinc 2012 come back bite as two side know hard young stupid time also feel like deserv feel like anyth illeg know hurt lot peopl two year period old shell former teen self big heart love gentl would anyth anyon rude immatur selfish person want eventu help peopl especi want apolog everyon hurt also want open old wound know want feel better move life time think happi move life would thank'\n",
      " ...\n",
      " 'lost job end last year insur run low afford pocket doctor chang came day plan pill left abl get though today straight full dose taper afford privat insur pocket cost plan could find even met 200 cover offic might well million right wait sinc approv suppos take day check last week said back could take day hear back went local free clinic told wait time see gener doctor long psychiatrist guarante actual see one today sweat stomach upset feel nauseou worri go bad withdraw brain told anoth person way get help say suicid even accord stori terrifi experi much want rather die way abl without insur feel hopeless sick safeti net stuck without help actual make feel would easier gave bad withdraw realli effect would like therapi anyth anyon abl take medic transit withdraw'\n",
      " 'fat ugli look guy tri date even work week one singl around age age'\n",
      " 'last sever came new low today let start way back 2013 back friend let call met class activ commun one stupid relat coupl ran one live state halfway across countri let call talk friendli time would play togeth everi night one night best friend log call show face hilari roll one peopl ever met later night friend look stalk profil second saw pictur beauti continu talk sever first talk 2013 overal grew close everi day one day 2014 realli nervou huge crush time dislik toward even state intim sever crush eventu got straight ask said well yeah dont want ruin said reason like current felt like date instead offici coupl 2014 may save enough money see weekend one best life never felt much passion love person abl see everi hard worth much teenag boy teenag stupid full august went concert one best girl made told immedi fit guilt right told stay probabl close trust awar piec excus tri say worri wors grew back togeth month stop talk got lone stupid got hot august met girl bit meet never found close said done reason stop talk never gave space peopl hurt still said kept peopl one girl say realli sex said tell right furiou believ still thought abl fix ruin everi chanc love would come back today told love never felt empti alon one tell feel like light insid gone think singl posit thought sinc know post guess weak hope mayb someon tell reason believ know']\n",
      "(4552, 2390)\n",
      "564\n",
      "(4552, 2390)\n",
      "(4552, 2390)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "4552\n",
      "['0' '0' '0' ... '0' '0' '0']\n",
      "['0' '0' '0' ... '0' '1' '0']\n",
      "0.8517526777020448\n",
      "[[3499    2]\n",
      " [ 607    0]]\n",
      "[3499    2  607    0]\n"
     ]
    }
   ],
   "source": [
    "#sgd\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  4552\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  3001\n",
      "['nobodi left someon els worst part suffer self day feel time feel empti feel stomach feel physic sensat hopeless brain battl want die hand know noth go ever chang tri act impuls everyth plain hard feel rage act public hard break cri walk tri show normal fall love someon hurt replac willingli chose block commun person talk someon els leav broke suicid knew like said want die choic point'\n",
      " 'hello look book read one self help get depress one enough get mind neg depress make sens thank'\n",
      " 'bad immatur old back huge free spirit know immatur age howev sinc 2012 come back bite as two side know hard young stupid time also feel like deserv feel like anyth illeg know hurt lot peopl two year period old shell former teen self big heart love gentl would anyth anyon rude immatur selfish person want eventu help peopl especi want apolog everyon hurt also want open old wound know want feel better move life time think happi move life would thank'\n",
      " ...\n",
      " 'lost job end last year insur run low afford pocket doctor chang came day plan pill left abl get though today straight full dose taper afford privat insur pocket cost plan could find even met 200 cover offic might well million right wait sinc approv suppos take day check last week said back could take day hear back went local free clinic told wait time see gener doctor long psychiatrist guarante actual see one today sweat stomach upset feel nauseou worri go bad withdraw brain told anoth person way get help say suicid even accord stori terrifi experi much want rather die way abl without insur feel hopeless sick safeti net stuck without help actual make feel would easier gave bad withdraw realli effect would like therapi anyth anyon abl take medic transit withdraw'\n",
      " 'fat ugli look guy tri date even work week one singl around age age'\n",
      " 'last sever came new low today let start way back 2013 back friend let call met class activ commun one stupid relat coupl ran one live state halfway across countri let call talk friendli time would play togeth everi night one night best friend log call show face hilari roll one peopl ever met later night friend look stalk profil second saw pictur beauti continu talk sever first talk 2013 overal grew close everi day one day 2014 realli nervou huge crush time dislik toward even state intim sever crush eventu got straight ask said well yeah dont want ruin said reason like current felt like date instead offici coupl 2014 may save enough money see weekend one best life never felt much passion love person abl see everi hard worth much teenag boy teenag stupid full august went concert one best girl made told immedi fit guilt right told stay probabl close trust awar piec excus tri say worri wors grew back togeth month stop talk got lone stupid got hot august met girl bit meet never found close said done reason stop talk never gave space peopl hurt still said kept peopl one girl say realli sex said tell right furiou believ still thought abl fix ruin everi chanc love would come back today told love never felt empti alon one tell feel like light insid gone think singl posit thought sinc know post guess weak hope mayb someon tell reason believ know']\n",
      "(4552, 2390)\n",
      "564\n",
      "(4552, 2390)\n",
      "(4552, 2390)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "4552\n",
      "['0' '0' '0' ... '0' '0' '0']\n",
      "['0' '0' '0' ... '0' '1' '0']\n",
      "0.8498052580331061\n",
      "[[3482   19]\n",
      " [ 598    9]]\n",
      "[3482   19  598    9]\n"
     ]
    }
   ],
   "source": [
    "#multinomialNB\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  4552\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  3001\n",
      "['nobodi left someon els worst part suffer self day feel time feel empti feel stomach feel physic sensat hopeless brain battl want die hand know noth go ever chang tri act impuls everyth plain hard feel rage act public hard break cri walk tri show normal fall love someon hurt replac willingli chose block commun person talk someon els leav broke suicid knew like said want die choic point'\n",
      " 'hello look book read one self help get depress one enough get mind neg depress make sens thank'\n",
      " 'bad immatur old back huge free spirit know immatur age howev sinc 2012 come back bite as two side know hard young stupid time also feel like deserv feel like anyth illeg know hurt lot peopl two year period old shell former teen self big heart love gentl would anyth anyon rude immatur selfish person want eventu help peopl especi want apolog everyon hurt also want open old wound know want feel better move life time think happi move life would thank'\n",
      " ...\n",
      " 'lost job end last year insur run low afford pocket doctor chang came day plan pill left abl get though today straight full dose taper afford privat insur pocket cost plan could find even met 200 cover offic might well million right wait sinc approv suppos take day check last week said back could take day hear back went local free clinic told wait time see gener doctor long psychiatrist guarante actual see one today sweat stomach upset feel nauseou worri go bad withdraw brain told anoth person way get help say suicid even accord stori terrifi experi much want rather die way abl without insur feel hopeless sick safeti net stuck without help actual make feel would easier gave bad withdraw realli effect would like therapi anyth anyon abl take medic transit withdraw'\n",
      " 'fat ugli look guy tri date even work week one singl around age age'\n",
      " 'last sever came new low today let start way back 2013 back friend let call met class activ commun one stupid relat coupl ran one live state halfway across countri let call talk friendli time would play togeth everi night one night best friend log call show face hilari roll one peopl ever met later night friend look stalk profil second saw pictur beauti continu talk sever first talk 2013 overal grew close everi day one day 2014 realli nervou huge crush time dislik toward even state intim sever crush eventu got straight ask said well yeah dont want ruin said reason like current felt like date instead offici coupl 2014 may save enough money see weekend one best life never felt much passion love person abl see everi hard worth much teenag boy teenag stupid full august went concert one best girl made told immedi fit guilt right told stay probabl close trust awar piec excus tri say worri wors grew back togeth month stop talk got lone stupid got hot august met girl bit meet never found close said done reason stop talk never gave space peopl hurt still said kept peopl one girl say realli sex said tell right furiou believ still thought abl fix ruin everi chanc love would come back today told love never felt empti alon one tell feel like light insid gone think singl posit thought sinc know post guess weak hope mayb someon tell reason believ know']\n",
      "(4552, 2390)\n",
      "564\n",
      "(4552, 2390)\n",
      "(4552, 2390)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "4552\n",
      "['0' '0' '0' ... '0' '0' '0']\n",
      "['0' '0' '0' ... '0' '1' '0']\n",
      "0.8339824732229796\n",
      "[[3394  107]\n",
      " [ 575   32]]\n",
      "[3394  107  575   32]\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  4552\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  3001\n",
      "['nobodi left someon els worst part suffer self day feel time feel empti feel stomach feel physic sensat hopeless brain battl want die hand know noth go ever chang tri act impuls everyth plain hard feel rage act public hard break cri walk tri show normal fall love someon hurt replac willingli chose block commun person talk someon els leav broke suicid knew like said want die choic point'\n",
      " 'hello look book read one self help get depress one enough get mind neg depress make sens thank'\n",
      " 'bad immatur old back huge free spirit know immatur age howev sinc 2012 come back bite as two side know hard young stupid time also feel like deserv feel like anyth illeg know hurt lot peopl two year period old shell former teen self big heart love gentl would anyth anyon rude immatur selfish person want eventu help peopl especi want apolog everyon hurt also want open old wound know want feel better move life time think happi move life would thank'\n",
      " ...\n",
      " 'lost job end last year insur run low afford pocket doctor chang came day plan pill left abl get though today straight full dose taper afford privat insur pocket cost plan could find even met 200 cover offic might well million right wait sinc approv suppos take day check last week said back could take day hear back went local free clinic told wait time see gener doctor long psychiatrist guarante actual see one today sweat stomach upset feel nauseou worri go bad withdraw brain told anoth person way get help say suicid even accord stori terrifi experi much want rather die way abl without insur feel hopeless sick safeti net stuck without help actual make feel would easier gave bad withdraw realli effect would like therapi anyth anyon abl take medic transit withdraw'\n",
      " 'fat ugli look guy tri date even work week one singl around age age'\n",
      " 'last sever came new low today let start way back 2013 back friend let call met class activ commun one stupid relat coupl ran one live state halfway across countri let call talk friendli time would play togeth everi night one night best friend log call show face hilari roll one peopl ever met later night friend look stalk profil second saw pictur beauti continu talk sever first talk 2013 overal grew close everi day one day 2014 realli nervou huge crush time dislik toward even state intim sever crush eventu got straight ask said well yeah dont want ruin said reason like current felt like date instead offici coupl 2014 may save enough money see weekend one best life never felt much passion love person abl see everi hard worth much teenag boy teenag stupid full august went concert one best girl made told immedi fit guilt right told stay probabl close trust awar piec excus tri say worri wors grew back togeth month stop talk got lone stupid got hot august met girl bit meet never found close said done reason stop talk never gave space peopl hurt still said kept peopl one girl say realli sex said tell right furiou believ still thought abl fix ruin everi chanc love would come back today told love never felt empti alon one tell feel like light insid gone think singl posit thought sinc know post guess weak hope mayb someon tell reason believ know']\n",
      "(4552, 2390)\n",
      "564\n",
      "(4552, 2390)\n",
      "(4552, 2390)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "4552\n",
      "['0' '0' '0' ... '0' '0' '0']\n",
      "['0' '0' '0' ... '0' '1' '0']\n",
      "0.8522395326192794\n",
      "[[3501    0]\n",
      " [ 607    0]]\n",
      "[3501    0  607    0]\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
