{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import gensim\n",
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import array, mean\n",
    "from gensim.test.utils import datapath\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from gensim import corpora, models\n",
    "#import sklearn.externals.joblib as extjoblib\n",
    "import joblib\n",
    "from sklearn import svm\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "def createTrainingDataset():\n",
    "    GISWDataLength = 1700\n",
    "    GIDataLength = 1800\n",
    "    count = 0\n",
    "    corpus = []\n",
    "    print(\"Extracting training data: \")\n",
    "    with open('cleanedGeneralIssuesCommonTS1.csv', mode='r') as commonReader:\n",
    "        commonReader.readline()\n",
    "        csvGISWReader = csv.reader(commonReader, delimiter=',')\n",
    "        for row in csvGISWReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1:\n",
    "                continue\n",
    "            corpus.append([1, row[1], string])\n",
    "            count += 1\n",
    "            if count > GISWDataLength:\n",
    "                break\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "\n",
    "    count = 0\n",
    "    with open('cleanedGeneralIssuesTS1.csv', mode='r') as generalReader:\n",
    "        generalReader.readline()\n",
    "        csvGIReader = csv.reader(generalReader, delimiter=',')\n",
    "        for row in csvGIReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1 or row[1] == \"[deleted]\":\n",
    "                continue\n",
    "            corpus.append([0, row[1], string])\n",
    "            count += 1\n",
    "            if count > GIDataLength:\n",
    "                break\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    random.shuffle(corpus)\n",
    "    print(\"SHUFFLING->>>>>>>>>>>>>\")\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    with open('training1.csv', mode='w') as writer:\n",
    "        csvWriter = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow(['Class', 'Author', 'Text'])\n",
    "        for row in corpus:\n",
    "            csvWriter.writerow(row)\n",
    "    print(\"Done Extracting\")\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingDataset():\n",
    "    GISWDataLength = 680\n",
    "    GIDataLength = 3500\n",
    "    count = 0\n",
    "    corpus = []\n",
    "    print(\"Extracting testing data: \")\n",
    "    with open('cleanedGeneralIssuesCommonTS2.csv', mode='r') as commonReader:\n",
    "        commonReader.readline()\n",
    "        csvGISWReader = csv.reader(commonReader, delimiter=',')\n",
    "        for row in csvGISWReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1:\n",
    "                continue\n",
    "\n",
    "            count += 1\n",
    "            #if (count < 240):\n",
    "            #    continue\n",
    "            corpus.append([1, row[1], string])\n",
    "            if count > GISWDataLength:\n",
    "                break\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "\n",
    "    count = 0\n",
    "    with open('cleanedGeneralIssuesTS2.csv', mode='r') as generalReader:\n",
    "        generalReader.readline()\n",
    "        csvGIReader = csv.reader(generalReader, delimiter=',')\n",
    "        for row in csvGIReader:\n",
    "            string = row[5]\n",
    "            string = [str(x) for x in string.strip().split()]\n",
    "            if len(string) <= 1 or row[1] == \"[deleted]\":\n",
    "                continue\n",
    "            count += 1\n",
    "            #if (count < 5000):\n",
    "            #   continue\n",
    "            corpus.append([0, row[1], string])\n",
    "            if count > GIDataLength :\n",
    "                break\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    random.shuffle(corpus)\n",
    "    print(\"SHUFFLING->>>>>>>>>>>>>\")\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "    for i in corpus[-5:]:\n",
    "        print(i)\n",
    "\n",
    "    with open('testing1.csv', mode='w') as writer:\n",
    "        csvWriter = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow(['Class', 'Author', 'Text'])\n",
    "        for row in corpus:\n",
    "            csvWriter.writerow(row)\n",
    "    print(\"Done Extracting\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModel():\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    with open('training1.csv', mode='r', encoding=\"utf-8\") as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=3:\n",
    "                corpus.append([str(x) for x in row[2].strip().split()])\n",
    "                docNumber += 1\n",
    "\n",
    "    print(\"Total Count: \", docNumber)\n",
    "    dictionary = gensim.corpora.Dictionary(corpus)\n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "    print(len(dictionary))\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=450)\n",
    "    print(len(dictionary))\n",
    "    #count = 0\n",
    "    #for k, v in dictionary.iteritems():\n",
    "     #   print(k, v)\n",
    "      #  count += 1\n",
    "       # if count > 10:\n",
    "        #    break\n",
    "    #print(\"corpus:\")\n",
    "    #print(corpus[1:100])\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    bow_corpus[1500]\n",
    "    #print(\"bow_corpus:\")\n",
    "    #print(bow_corpus[1:100])\n",
    "    \n",
    "    bow_doc_4310=bow_corpus[1500]\n",
    "    for i in range(len(bow_doc_4310)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],\n",
    "                                                         dictionary[bow_doc_4310[i][0]],\n",
    "                                                         bow_doc_4310[i][1]))\n",
    "    \n",
    "    \"\"\"tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    from pprint import pprint\n",
    "    for doc in corpus_tfidf:\n",
    "        pprint(doc)\n",
    "        break\n",
    "    \"\"\"\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=2, id2word=dictionary, passes=10, workers=4)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "        \n",
    "        \n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    with open('testing1.csv', mode='r', encoding=\"utf-8\") as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=3:\n",
    "                corpus.append([str(x) for x in row[2].strip().split()])\n",
    "                docNumber += 1\n",
    "\n",
    "    print(\"Total Count: \", docNumber)\n",
    "    dictionary = gensim.corpora.Dictionary(corpus)\n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "    print(len(dictionary))\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=450)\n",
    "    print(len(dictionary))\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    #bow_corpus[5912]\n",
    "    #print(\"bow_corpus:\")\n",
    "    #print(bow_corpus[1:100])\n",
    "    \n",
    "    bow_doc_4310=bow_corpus[770]\n",
    "    for i in range(len(bow_doc_4310)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],\n",
    "                                                         dictionary[bow_doc_4310[i][0]],\n",
    "                                                         bow_doc_4310[i][1]))\n",
    "    \n",
    "   \n",
    "    print(lda_model.get_document_topics(bow_doc_4310)) \n",
    "    \"\"\"    \n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=1, workers=4)\n",
    "    \n",
    "    for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "        print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    corpus[5912]\n",
    "    for index, score in sorted(lda_model[bow_corpus[5912]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    \n",
    "    for index, score in sorted(lda_model_tfidf[bow_corpus[5912]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "    # save the model to disk\n",
    "    #modelFile = datapath('/Users/Dell/final major project/ldaTrainedModel')\n",
    "    #lda_model.save(modelFile)\n",
    "    #filename = 'finalized_model.sav'\n",
    "    #pickle.dump(model, open(filename, 'wb'))\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestTopicModel():\n",
    "    searchParams = {'n_components': [n for n in range(2, 7)], 'learning_decay': [0.3,0.5,0.7,0.9]}\n",
    "    lda = LatentDirichletAllocation()\n",
    "    model = GridSearchCV(lda, searchParams)\n",
    "\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    with open('training1.csv', mode='r') as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                if (int(row[0]) == 1):\n",
    "                    corpus.append(\" \".join(ast.literal_eval(row[2])))\n",
    "                    docNumber += 1\n",
    "    print(\"Total Doc: \", docNumber)\n",
    "    for i in corpus[0:5]:\n",
    "        print(i)\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=10)\n",
    "    data_vectorized = vectorizer.fit_transform(corpus)\n",
    "    print('Started Training')\n",
    "    model.fit(data_vectorized)\n",
    "    print('Found the best model')\n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "    # Log Likelihood Score\n",
    "    print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "    # Perplexity\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "    filename = 'finalized_LDAModel.sav'\n",
    "    pickle.dump(best_lda_model, open(filename, 'wb'))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainFeatures(filename):\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    suicidalCount = 0\n",
    "    nonsuicidalCount = 0\n",
    "    with open(filename, mode='r') as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                corpus.append(array([row[0], \" \".join(ast.literal_eval(row[2]))]))\n",
    "                if (row[0] == '1'):\n",
    "                    suicidalCount += 1\n",
    "                else:\n",
    "                    nonsuicidalCount += 1\n",
    "                docNumber += 1\n",
    "    print(\"Total Doc: \", docNumber)\n",
    "    print(\"Suicidal Count: \", suicidalCount)\n",
    "    print(\"Non-Suicidal Count: \", nonsuicidalCount)\n",
    "    corpus = array(corpus)\n",
    "    print(corpus[:,1])\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=10)\n",
    "    data_vectorized = vectorizer.fit_transform(corpus[:, 1])\n",
    "    print(data_vectorized.shape)\n",
    "    print(vectorizer.vocabulary_.get('depress'))\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(data_vectorized)\n",
    "    X_train_tf = tf_transformer.transform(data_vectorized)\n",
    "    print(X_train_tf.shape)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(data_vectorized)\n",
    "    print(X_train_tfidf.shape)\n",
    "\n",
    "    return X_train_tfidf, corpus[:, 0], vectorizer, tfidf_transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestFeatures(filename):\n",
    "    corpus = []\n",
    "    docNumber = 0\n",
    "    suicidalCount = 0\n",
    "    nonsuicidalCount = 0\n",
    "    with open(filename, mode='r') as reader:\n",
    "        reader.readline()\n",
    "        csvReader = csv.reader(reader)\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                corpus.append(array([row[0], \" \".join(ast.literal_eval(row[2]))]))\n",
    "                if(row[0] == '1'):\n",
    "                    suicidalCount += 1\n",
    "                else:\n",
    "                    nonsuicidalCount += 1\n",
    "                docNumber += 1\n",
    "    print(\"Total Doc: \", docNumber)\n",
    "    print(\"Suicidal Count: \", suicidalCount)\n",
    "    print(\"Non-Suicidal Count: \", nonsuicidalCount)\n",
    "    corpus = array(corpus)\n",
    "    return corpus\n",
    "    # print(corpus[:,1])\n",
    "\n",
    "    # vectorizer = CountVectorizer(min_df=10)\n",
    "    # data_vectorized = vectorizer.transform(corpus[:, 1])\n",
    "    # print(data_vectorized.shape)\n",
    "    # print(vectorizer.vocabulary_.get('depress'))\n",
    "    #\n",
    "    # tf_transformer = TfidfTransformer(use_idf=False).fit(data_vectorized)\n",
    "    # X_train_tf = tf_transformer.transform(data_vectorized)\n",
    "    # print(X_train_tf.shape)\n",
    "    #\n",
    "    # tfidf_transformer = TfidfTransformer()\n",
    "    # X_train_tfidf = tfidf_transformer.fit_transform(data_vectorized)\n",
    "    # print(X_train_tfidf.shape)\n",
    "\n",
    "    # return X_train_tfidf, corpus[:, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM():\n",
    "    X_train_tfidf, target, countVectorizer, tfidf_transformer = getTrainFeatures('training1.csv')\n",
    "    print('Started Training...')\n",
    "    #clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=500, tol=None)\n",
    "    #clf = MultinomialNB()\n",
    "    #clf = svm.SVC(kernel='rbf')\n",
    "    clf = RandomForestClassifier(max_depth = 6, random_state=0)\n",
    "    clf.fit(X_train_tfidf, target)\n",
    "    print('Done Training...')\n",
    "    print('Started Testing...')\n",
    "\n",
    "    testingCorpus = getTestFeatures('testing1.csv')\n",
    "    X_test_counts = countVectorizer.transform(testingCorpus[:, 1])\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "    print(len(target))\n",
    "    predictions = clf.predict(X_test_tfidf)\n",
    "    print(predictions)\n",
    "    print(target)\n",
    "    print(mean(predictions == testingCorpus[:, 0]))\n",
    "    print(confusion_matrix(testingCorpus[:, 0], predictions))\n",
    "    print(confusion_matrix(testingCorpus[:, 0], predictions).ravel())\n",
    "    \n",
    "    \"\"\"X_set,Y_set=X_test_tfidf,testingCorpus[:, 0]\n",
    "    X1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n",
    "                  np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\n",
    "    plt.contourf(X1,X2,clf.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n",
    "                 alpha=0.75,cmap=ListedColormap(('red','green')))\n",
    "    plt.xlim(X1.min(),X1.max())\n",
    "    plt.ylim(X2.min(),X2.max())\n",
    "    for i,j in enumerate(np.unique(Y_set)):\n",
    "        plt.scatter(X_set[Y_set==j,0],X_set[Y_set==j,1],\n",
    "                c=ListedColormap(('red','green'))(i),label=j)\n",
    "                      \n",
    "    plt.title('Random Forest Classifier(Test set)')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.legend()\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"validatingCorpus = getTestFeatures('validation1.csv')\n",
    "    X_test_counts = countVectorizer.transform(validatingCorpus[:, 1])\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "    # print(len(test_target))\n",
    "    # predictions = clf.predict(X_test_tfidf)\n",
    "    print(predictions)\n",
    "    # print(test_target)\n",
    "    print(mean(predictions == validatingCorpus[:, 0]))\n",
    "    print(confusion_matrix(validatingCorpus[:, 0], predictions))\n",
    "    print(confusion_matrix(validatingCorpus[:, 0], predictions).ravel())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training data: \n",
      "[1, 'Plassholdertekst', ['feel', 'like', 'actual', 'feel', 'normal', 'happi', 'fall', 'hard', 'happi', 'alway', 'part', 'cycl', 'depress']]\n",
      "[1, 'Limnir', ['constantli', 'wish', 'peopl', 'dead', 'cancer', 'recent', 'older', 'stepbroth', 'live', 'sick', 'constantli', 'tell', 'younger', 'brother', 'wish', 'dead', 'never', 'second', 'neither', 'say', 'like', 'around', 'time', 'sick', 'wish', 'peopl', 'dead', 'time', 'say', 'face', 'someth', 'like', 'work', 'one', 'thing', 'time', 'everyon', 'els', 'littl', 'brother', 'time', 'say', 'turn', 'peopl', 'anyth', 'littl', 'fix', 'broken', 'time', 'everyth', 'tell', 'wish', 'kill', 'older', 'stepbroth', 'say', 'pay', 'hous', 'noth', 'brother', 'think', 'sick', 'think', 'normal', 'wish', 'littl', 'brother', 'dead']]\n",
      "[1, 'Plassholdertekst', ['often', 'final', 'feel', 'realli', 'happi', 'good', 'suddenli', 'realli', 'bad', 'could', 'mean', 'bipolar', 'type', 'happi', 'real']]\n",
      "[1, 'SHThrowAway213', ['let', 'know', 'said', 'girl', 'said', 'keep', 'eye', 'still', 'realli', 'coupl', 'sinc', 'told', 'panic', 'alreadi', 'get', 'intens', 'help', 'want', 'anyon', 'els', 'gone', 'hell', 'way', 'keep', 'anyon', 'els', 'safe', 'polic', 'help', 'reluct', 'fault', 'anyon', 'els']]\n",
      "[1, 'SHThrowAway213', ['went', 'well', 'met', 'around', 'hour', 'coffe', 'got', 'realli', 'well', 'great', 'convers', 'live', 'town', 'cute', 'realli', 'nice', 'funni', 'gorgeou', 'realli', 'nice', 'person', 'quit', 'shi', 'first', 'tri', 'shi', 'said', 'could', 'tell', 'meet', 'nice', 'walk', 'good', 'go', 'scare', 'away', 'mental', 'health', 'chang', 'good', 'bad', 'know', 'tell', 'hope', 'scare']]\n",
      "[0, 'foxlinery', ['fifteen', 'year', 'old', 'atheist', 'take', 'psychiatrist', 'consid', 'depress', 'match', 'bipolar', 'disord', 'home', 'school', 'get', 'good', 'usual', 'score', 'averag', 'social', 'horribl', 'definit', 'tell', 'comfort', 'social', 'poor', 'subsequ', 'one', 'well', 'father', 'bipolar', 'mother', 'mental', 'ill', 'know', 'normal', 'matter', 'father', 'strict', 'polici', 'must', 'long', 'hair', 'later', 'young', 'age', 'form', 'mostli', 'trivial', 'done', 'would', 'warrant', 'time', 'around', 'think', 'around', 'time', 'test', 'school', 'someth', 'dad', 'came', 'home', 'say', 'failur', 'would', 'never', 'anyth', 'life', 'night', 'run', 'away', 'home', 'next', 'incid', 'work', 'school', 'mother', 'absolut', 'lost', 'rush', 'kitchen', 'tear', 'plastic', 'knife', 'yell', 'think', 'start', 'dramat', 'frequent', 'around', 'time', 'point', 'rememb', 'becom', 'realli', 'detach', 'actual', 'slightli', 'time', 'gotten', 'room', 'upstair', 'left', 'good', 'bit', 'time', 'built', 'constantli', 'around', 'peopl', 'poorli', 'got', 'point', 'complet', 'forgot', 'done', 'point', 'church', 'close', 'start', 'friendship', 'person', 'commun', 'feel', 'like', 'atheist', 'good', 'job', 'convinc', 'religion', 'previous', 'complet', 'dog', 'know', 'religi', 'peopl', 'horribl', 'church', 'peopl', 'associ', 'basic', 'god', 'go', 'heaven', 'small', 'amount', 'mani', 'easili', 'put', 'found', 'atheist', 'say', 'least', 'say', 'evil', 'away', 'church', 'youth', 'want', 'get', 'reject', 'soon', 'reveal', 'lot', 'physic', 'abus', 'serious', 'bloodi', 'church', 'tortur', 'everi', 'pure', 'solid', 'question', 'saniti', 'break', 'depress', 'get', 'frequent', 'sever', 'longer', 'comput', 'strictli', 'live', 'room', 'mother', 'monitor', 'time', 'bare', 'room', 'breath', 'close', 'fix', 'feel', 'wrong', 'even', 'feel', 'like', 'situat', 'bad', 'point', 'enough', 'angri', 'sorrow', 'one', 'real', 'life', 'one', 'whini', 'bitch', 'edit', 'male']]\n",
      "[0, 'jordaniac89', ['tri', 'get', 'get', 'hous', 'sever', 'suppos', 'event', 'even', 'stand', 'room', 'wrestl', 'decid', 'realiz', 'go', 'get', 'better', 'know', 'want', 'go', 'happen', 'told', 'want', 'wast', 'time', 'never', 'asham', 'hate', 'hate', 'life', 'let', 'famili', 'terribl', 'owner', 'dog', 'go', 'get', 'better']]\n",
      "[0, 'singing016', ['charact', 'sex', 'male', 'level', 'employ', 'unemploy', 'live', 'mental', 'state', 'log', 'empti']]\n",
      "[0, 'A_Losing_Game', ['three', 'work', 'like', 'dog', 'starv', 'like', 'one', 'suicid', 'unfortun', 'sinc', 'natur', 'desir', 'live', 'go', 'one', 'first', 'two', 'pretti', 'sad', 'peopl', 'peopl', 'everi', 'week', 'wait', 'come', 'spend', 'five', 'day', 'week', 'wait', 'two', 'even', 'anyth', 'good', 'curs', 'world', 'get', 'everi', 'morn', 'live', 'life', 'work']]\n",
      "[0, 'mjuly', ['mid', 'get', 'marri', 'love', 'life', 'alway', 'thought', 'child', 'love', 'work', 'sister', 'younger', 'ador', 'longer', 'dealt', 'mental', 'ill', 'depress', 'anxieti', 'want', 'spend', 'much', 'time', 'think', 'would', 'pregnanc', 'alright', 'stay', 'depress', 'anxieti', 'make', 'bad', 'mother', 'make', 'labor', 'anyon', 'anyon', 'advic', 'want', 'famili', 'doubt']]\n",
      "SHUFFLING->>>>>>>>>>>>>\n",
      "[1, 'wtfgiraffe18', ['financi', 'situat', 'caus', 'problem', 'get', 'help', 'never', 'treatment', 'hospit', 'go', 'pay', 'seem', 'possibl', 'insur', 'know', 'cover', 'everyth']]\n",
      "[0, 'another_exmoosegal', ['new', 'may', 'seen', 'previou', 'question', 'although', 'bit', 'differ', 'normal', 'shi', 'person', 'social', 'anxieti', 'bad', 'find', 'unabl', 'mani', 'feel', 'like', 'could', 'achiev', 'much', 'certain', 'hold', 'back', 'whenev', 'around', 'peopl', 'even', 'close', 'famili', 'find', 'hard', 'talk', 'without', 'shake', 'particular', 'aw', 'head', 'move', 'side', 'side', 'gotten', 'much', 'wors', 'late', 'turn', 'side', 'anoth', 'person', 'like', 'head', 'unabl', 'move', 'twitch', 'embarrass', 'even', 'class', 'use', 'answer', 'alway', 'upset', 'thing', 'realli', 'hold', 'back', 'mani', 'anoth', 'thing', 'alway', 'awkward', 'eye', 'contact', 'long', 'time', 'alway', 'end', 'cough', 'hair', 'even', 'feel', 'nervou', 'around', 'peopl', 'feel', 'peopl', 'even', 'feel', 'confid', 'even', 'anyon', 'els', 'get', 'weird', 'head', 'could', 'never', 'find', 'anyth', 'anyon', 'could', 'tri', 'greatli']]\n",
      "[0, 'ich_bin_lero', ['work', 'list', 'believ', 'stabl', 'true', 'life', 'stay', 'matter', 'someth', 'like', 'talent', 'music', 'learn', 'someth', 'new', 'realli', 'fast', 'realli', 'import', 'apart', 'clean', 'love', 'stuff', 'like', 'music', 'ador', 'musician', 'person', 'make', 'good', 'first', 'impress', 'meet', 'new', 'peopl', 'believ', 'god', 'believ', 'god', 'hope', 'would', 'help', 'identifi', 'respons', 'come', 'work', 'peopl', 'count', 'much', 'differ', 'stuff', 'hope', 'help', 'littl', 'know', 'life', 'never', 'chang']]\n",
      "[1, 'WittyPancake', ['societi', 'one', 'memori', 'alway', 'walk', 'talk', 'know', 'realli', 'weird', 'former', 'crush', 'said', 'peopl', 'alway', 'use', 'sit', 'next', 'lunch', 'talk', 'laugh', 'peopl', 'would', 'ask', 'knew', 'joke', 'even', 'would', 'join', 'teas']]\n",
      "[0, 'anrose', ['guess', 'someon', 'would', 'call', 'grey', 'area', 'assault', 'know', 'met', 'summer', 'program', 'colleg', 'knew', 'met', 'one', 'night', 'left', 'next', 'morn', 'origin', 'poetri', 'larg', 'group', 'eventu', 'talk', 'mental', 'ill', 'respect', 'depress', 'experi', 'schizophrenia', 'relationship', 'time', 'told', 'realli', 'understand', 'depress', 'practic', 'room', 'guitar', 'alon', 'red', 'right', 'went', 'friend', 'mine', 'come', 'anoth', 'red', 'bit', 'told', 'friend', 'fine', 'alon', 'resist', 'left', 'felt', 'first', 'time', 'like', 'someon', 'understood', 'go', 'cut', 'chase', 'wrong', 'relationship', 'guy', 'know', 'post', 'go', 'farther', 'away', 'tri', 'sever', 'time', 'time', 'away', 'said', 'say', 'guess', 'kind', 'said', 'way', 'mayb', 'thought', 'meant', 'wrong', 'even', 'though', 'want', 'instead', 'wrong', 'want', 'kept', 'say', 'like', 'know', 'make', 'feel', 'good', 'deserv', 'feel', 'good', 'anyth', 'think', 'deserv', 'happi', 'eventu', 'got', 'tire', 'resist', 'let', 'said', 'sure', 'said', 'felt', 'like', 'person', 'way', 'make', 'feel', 'like', 'good', 'person', 'make', 'feel', 'good', 'later', 'said', 'stop', 'said', 'feel', 'guilti', 'finish', 'probabl', 'cut', 'stop', 'put', 'hand', 'said', 'bed', 'said', 'blue', 'say', 'aggress', 'plead', 'guess', 'stay', 'head', 'fight', 'littl', 'bit', 'eventu', 'said', 'roommat', 'would', 'worri', 'saw', 'next', 'morn', 'left', 'cut', 'anyway', 'told', 'stori', 'sexual', 'reason', 'big', 'advoc', 'consent', 'confus', 'long', 'time', 'thought', 'sinc', 'right', 'figur', 'might', 'grey', 'area', 'alreadi', 'long', 'sinc', 'incid', 'decid', 'take', 'action', 'went', 'west', 'coast', 'univers', 'fall', 'transfer', 'colleg', 'follow', 'year', 'still', 'finish', 'last', 'year', 'high', 'school', 'appli', 'colleg', 'school', 'colleg', 'past', 'fall', 'small', 'school', 'class', 'live', 'build', 'pa', 'lot', 'lot', 'mutual', 'eat', 'time', 'made', 'contact', 'otherwis', 'year', 'realli', 'know', 'anyth', 'point', 'matter', 'close', 'know', 'associ', 'see', 'much', 'even', 'school', 'page', 'anyon', 'would', 'want', 'join', 'concert', 'benefit', 'sexual', 'assault', 'like', 'made', 'face', 'hot', 'felt', 'like', 'post', 'see', 'send', 'kind', 'messag', 'even', 'though', 'probabl', 'case', 'part', 'destroy', 'tell', 'everyon', 'anoth', 'part', 'block', 'everi', 'social', 'medium', 'never', 'mention', 'name', 'pretend', 'never', 'anoth', 'part', 'brave', 'enough', 'block', 'keep', 'head', 'high', 'make', 'scene', 'want', 'advic', 'someth', 'mani', 'prais', 'sexual', 'assault', 'know', 'lie', 'see', 'say', 'glad', 'support', 'peopl', 'like', 'hurt', 'peopl', 'like', 'hurt', 'guy', 'go', 'colleg', 'alway', 'peopl', 'much', 'sexual', 'assault', 'think', 'sexual', 'ago', 'edit', 'clariti']]\n",
      "[1, 'pikapikachoo', ['want', 'job', 'miser', 'guy', 'work', 'corpor', 'custom', 'servic', 'noth', 'area', 'would', 'even', 'tech', 'support', 'live', 'energi', 'pa', 'time', 'actual', 'live', 'life', 'would', 'hurt', 'anyon', 'left', 'would', 'easiest', 'way', 'get', 'done']]\n",
      "[1, 'Thisdudeadam', ['littl', 'back', 'stori', 'grow', 'physic', 'beaten', 'father', 'mani', 'mani', 'sexual', 'uncl', 'father', 'put', 'mother', 'wall', 'knock', 'unconsci', 'put', 'gun', 'knife', 'head', 'throat', 'among', 'mani', 'talk', 'anyon', 'due', 'incred', 'togeth', 'last', 'year', '2015', 'day', 'gave', 'birth', 'beauti', 'daughter', 'love', 'anyth', 'entir', 'world', 'gave', 'birth', 'decid', 'back', 'work', 'decis', 'made', 'decid', 'best', 'stay', 'home', 'dad', 'fast', 'forward', 'later', 'say', 'unhappi', 'caus', 'alway', 'behind', 'spend', 'enough', 'time', 'anyth', 'togeth', 'fan', 'famili', 'gener', 'stay', 'home', 'past', 'depress', 'realli', 'realli', 'bad', 'say', 'anyth', 'still', 'feel', 'asham', 'life', 'thrown', 'video', 'way', 'escap', 'go', 'head', 'took', 'care', 'daughter', 'everyday', 'free', 'time', 'longer', 'fun', 'distract', 'world', 'consider', 'without', 'around', 'vibrant', 'sweet', 'soul', 'told', 'therapi', 'pay', 'coupl', 'therapi', 'anyth', 'fix', 'relationship', 'say', 'longer', 'cold', 'person', 'talk', 'regard', 'daughter', 'time', 'realli', 'talk', 'alway', 'anyth', 'everyth', 'els', 'broke', 'week', 'ago', 'agre', 'work', 'see', 'last', 'night', 'want', 'get', 'marri', 'next', 'year', 'leav', 'happi', 'think', 'go', 'realli', 'tri', 'fix', 'morn', 'love', 'tri', 'alway', 'thought', 'mouth', 'end', 'time', 'cruel', 'world', 'think', 'know', 'like', 'father', 'grow', 'want', 'daughter', 'realli', 'realli', 'wish', 'someth', 'would', 'happen', 'take', 'famili', 'left', 'mother', 'talk', 'time', 'week', 'realli', 'close', 'hand', 'know', 'go', 'embarrass', 'asham', 'everyth', 'said', 'post', 'tell', 'go', 'head', 'realli', 'feel']]\n",
      "[1, 'DarkFox21', ['met', 'girl', 'year', 'ago', 'amaz', 'bodi', 'funni', 'awesom', 'sex', 'multipl', 'time', 'cours', 'happi', 'idea', 'found', 'crap', 'stay', 'far', 'away', 'ran', 'bloodi', 'nose', 'black', 'eye', 'cri', 'say', 'hit', 'rage', 'place', 'broke', 'nose', 'ever', 'fight', 'worth', 'assault', 'charg', 'commun', 'servic', 'later', 'find', 'actual', 'guy', 'see', 'behind', 'back', 'teeth', 'get', 'away', 'told', 'never', 'see', 'depress', 'anxieti', 'long', 'met', 'took', 'hard', 'one', 'person', 'ever', 'fought', 'away', 'tri', 'talk', 'time', 'away', 'week', 'ago', 'street', 'lip', 'told', 'one', 'almost', 'took', 'dig', 'around', 'found', 'true', 'get', 'six', 'jail', 'talk', 'steadili', 'sinc', 'date', 'tomorrow', 'know', 'whether', 'trust', 'realli', 'like', 'want', 'know', 'anoth', 'big', 'lie', 'bad', 'come', 'way', 'final', 'could', 'big', 'time', 'would', 'like', 'advic', 'fresh', 'tri', 'make', 'work', 'updat', 'show', 'date', 'stop', 'talk', 'think', 'go', 'work', 'thank', 'advic', 'though', 'much']]\n",
      "[1, '8a67b5309', ['think', 'peopl', 'depress', 'vice']]\n",
      "[1, 'TaurineLine719', ['think', 'that']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Extracting\n",
      "Extracting testing data: \n",
      "[1, 'WampuClawGirl', ['one', 'feel', 'like', 'pressur', 'push', 'bed', 'clue', 'could', 'imagin', 'yeah', 'sort', 'heavi', 'especi', 'explain', 'point', 'difficulti', 'move', 'like', 'extra', 'even', 'though', 'fall', 'asleep', 'number', 'mainli', 'music', 'calm', 'afraid', 'random', 'would', 'hear', 'otherwis', 'timer', 'set', 'one', 'hour', 'music', 'usual', 'work', 'sinc', 'fall', 'asleep', 'within', 'time', 'late', 'sleep', 'head', 'fear', 'see', 'someth', 'scari', 'room', 'someon', 'break', 'murder', 'mention', 'hear', 'weird', 'seem', 'come', 'music', 'elsewher', 'almost', 'like', 'someon', 'talk', 'music', 'deep', 'background', 'voic', 'often', 'hear', 'well', 'sound', 'like', 'come', 'though', 'part', 'song', 'listen', 'usual', 'happen', 'last', 'night', 'troubl', 'breath', 'well', 'clue', 'mayb', 'hell', 'know', 'go', 'crazi', 'would', 'appreci', 'weigh', 'lack', 'sleep', 'start', 'take', 'toll', 'troubl', 'throughout', 'day', 'lay', 'bed', 'right', 'want', 'work', 'stori', 'energi', 'even', 'think']]\n",
      "[1, '2222lil', ['happi', 'know', 'want', 'dead', 'alreadi', 'know']]\n",
      "[1, 'angryguy4444', ['depress', 'two', 'major', 'depress', 'two', 'half', 'ago', 'went', 'bit', 'need', 'get', 'bed', 'get', 'bed', 'noon', 'sport', 'still', 'talk', 'peopl', 'still', 'work', 'went', 'colleg', 'got', 'engin', 'degre', 'nasti', 'fight', 'drug', 'help', 'depress', 'alon', 'want', 'make', 'appoint', 'doc', 'reason', 'simpl', 'function', 'normal', 'know', 'charact', 'sick', 'tell', 'doctor', 'everyth', 'go', 'tell', 'hopeless', 'loser', 'fine', 'normal', 'human', 'sick', 'post', 'need', 'commit', 'wrote', 'must', 'keep', 'thank', 'read', 'updat', 'thing', 'keep', 'guess', 'depress', 'tell', 'sever', 'depress', 'might', 'accur', 'trustworthi', 'depress', 'trust', 'like', 'feel', 'good', 'told', 'sever', 'depress', 'score', 'even', 'wors', 'half', 'ago', 'depress', 'first', 'stroke', 'yet', 'feel', 'silli', 'help', 'must', 'noth', 'must', 'lazi', 'sure', 'noth', 'seriou', 'highest', 'convinc', 'actual', 'worth', 'schedul', 'appoint', 'mean', 'sure', 'noth', 'sure', 'noth', 'worri', 'right', 'right', 'learn', 'live', 'learn', 'surviv', 'work', 'function', 'normal', 'realli', 'live']]\n",
      "[1, 'pokesomi', ['hell', 'food', 'want', 'eat', 'hate', 'depress', 'love', 'feel', 'complet', 'worthless']]\n",
      "[1, 'Plsanswerawkwardporn', ['also', 'kind', 'afraid', 'cut', 'sure', 'steril', 'blade', 'know']]\n",
      "[0, 'doogonl', ['like', 'charact', 'creepi', 'hire', 'mostli', 'make', 'trap', 'giant', 'catch', 'rape', 'bite', 'final', 'kill', 'sometim', 'one', 'tri', 'leav', 'earli', 'run', 'front', 'lock', 'get', 'away', 'caught', 'eventu', 'nervou', 'go', 'jail', 'jail', 'offic', 'tire', 'nake', 'tree', 'field', 'littl', 'like', 'sad', 'cri', 'come', 'back', 'sleep', 'play', 'send', 'also', 'someon', 'made', 'bang', 'accur', 'needl', 'say', 'felt', 'like', 'hot', 'also', 'creepi', 'look', 'cloth']]\n",
      "[0, 'ImpelResonate', ['everi', 'semest', 'colleg', 'dump', 'question', 'amount', 'time', 'tri', 'pa', 'class', 'time', 'bare', 'pa', 'pa', 'everi', 'time', 'fail', 'extrem', 'make', 'sure', 'focu', 'school', 'someth', 'equival', 'extra', 'feel', 'depress', 'feel', 'depress', 'know', 'incap', 'alon', 'without', 'extra', 'current', 'tri', 'learn', 'digit', 'artist', 'feel', 'like', 'skill', 'hit', 'realli', 'low', 'peak', 'seem', 'improv', 'learn', 'like', 'self', 'taught', 'artist', 'feel', 'art', 'still', 'realli', 'know', 'accept', 'hopeless', 'caus', 'go', 'need', 'constant', 'guidanc', 'rest', 'life']]\n",
      "[0, 'dolcebellaluna', ['final', 'come', 'admit', 'major', 'neg', 'attent', 'seek', 'went', 'group', 'ago', 'feel', 'uncomfort', 'differ', 'place', 'life', 'think', 'readi', 'tri', 'group', 'therapi', 'feedback', 'posit', 'hour', 'long', 'intak', 'next', 'week', 'two', 'hour', 'long', 'group', 'session']]\n",
      "[0, 'randomdude867', ['hit', 'sophomor', 'year', 'high', 'school', 'fine', 'life', 'part', 'happi', 'hit', 'sophomor', 'year', 'everyth', 'chang', 'least', 'think', 'know', 'post', 'felt', 'like', 'alright', 'everyon', 'like', 'actual', 'give', 'hit', 'sophomor', 'year', 'realiz', 'realiz', 'keep', 'pleas', 'dad', 'peopl', 'know', 'mean', 'also', 'group', 'middl', 'school', 'make', 'girl', 'would', 'look', 'direct', 'twice', 'know', 'talk', 'new', 'peopl', 'like', 'person', 'care', 'care', 'brother', 'worst', 'part', 'brother', 'advantag', 'money', 'dad', 'turn', 'huge', 'fight', 'one', 'left', 'cri', 'room', 'want', 'anyth', 'colleg', 'want', 'pursu', 'slip', 'due', 'dad', 'worthless', 'piec', 'last', 'year', 'shown', 'anybodi', 'anyth', 'year', 'mostli', 'worst', 'part', 'pick', 'smoke', 'ago', 'know', 'take', 'physic', 'perform', 'think', 'take', 'someon', 'told', 'break', 'come', 'think', 'endur', 'one', 'year', 'get', 'hell', 'kill', 'ever', 'sinc', 'feel', 'way', 'know', 'go', 'know', 'happi', 'get', 'everyday', 'mind', 'blank', 'school', 'come', 'home', 'thing', 'sane', 'gym', 'hour', 'sleep', 'endlessli', 'better', 'next', 'day', 'never', 'depress', 'hate']]\n",
      "[0, 'Dylamations', ['right', 'social', 'anxieti', 'stuff', 'stuff', 'time', 'slowli', 'get', 'better', 'occasion', 'problem', 'becom', 'common', 'race', 'could', 'overthink', 'liter', 'anyth', 'point', 'give', 'headach', 'life', 'someth', 'school', 'even', 'someth', 'dumb', 'like', 'video', 'game', 'someth', 'get', 'anxiou', 'much', 'le', 'use', 'much', 'wors', 'sick', 'think', 'want', 'cri', 'right', 'caus', 'want', 'happi', 'life', 'afraid', 'say', 'famili', 'afraid', 'say', 'anyth', 'fear', 'think', 'go', 'crazi', 'alreadi', 'much', 'want', 'tell', 'counselor', 'drastic', 'chang', 'direct', 'limit', 'number', 'session', 'free', 'servic', 'know']]\n",
      "SHUFFLING->>>>>>>>>>>>>\n",
      "[0, 'fallenhero22', ['first', 'time', 'dose', 'look', 'see', 'much', 'littl']]\n",
      "[0, 'unquietspirits', ['deal', 'mental', 'ever', 'sinc', 'around', 'ten', 'old', 'get', 'wors', 'wors', 'time', 'go', 'mental', 'ill', 'along', 'lost', 'famili', 'spend', 'time', 'solitud', 'two', 'left', 'rare', 'talk', 'basic', 'never', 'see', 'late', 'want', 'affect', 'feel', 'skip', 'class', 'two', 'sad', 'move', 'alon', 'struggl', 'broke', 'get', 'therapi', 'anim', 'sustain', 'physic', 'chronic', 'two', 'seem', 'worri', 'get', 'wrong', 'fantast', 'love', 'love', 'strongli', 'love', 'realli', 'help', 'scare', 'realli', 'feel', 'like', 'realli', 'need', 'affect', 'care', 'get', 'depress', 'current', 'miser', 'ever', 'entir', 'life', 'anyth', 'sever', 'depress', 'complet', 'isol', 'due', 'social', 'almost', 'money', 'get', 'help', 'desper', 'need', 'help', 'reach', 'danger', 'place', 'get', 'affect', 'without']]\n",
      "[0, 'Upvote_Responsibly', ['anxieti', 'idea', 'life', 'suck', 'work', 'problem', 'run', 'endless', 'time', 'depress', 'self', 'loath', 'doubt', 'determin', 'idea', 'fail', 'think', 'way', 'problem', 'run', 'endless', 'depress', 'anxiou', 'mind', 'constantli', 'simultan', 'look', 'never', 'time', 'rest', 'reason', 'intern', 'battl', 'overwhelm', 'exhaust', 'depress', 'peopl', 'feel', 'way', 'suicid', 'suicid', 'best', 'resolv', 'feel', 'noth', 'hell', 'lot', 'better', 'deal', 'battl', 'time']]\n",
      "[0, 'sad_sad_homo', ['see', 'even', 'explain', 'stare', 'box', 'tri', 'write', 'someth', 'coher', 'even', 'simpl', 'overwhelm', 'time', 'feel', 'like', 'mind', 'blank', 'noth', 'go', 'wast', 'human', 'bodi']]\n",
      "[0, 'FuckedLastAccountLOL', ['suicid', 'sinc', 'suicid', 'like', 'part', 'imagin', 'like', 'look', 'high', 'build', 'coupl', 'much', 'would', 'hurt', 'fall', 'high', 'enough', 'die', 'like', 'dude', 'point', 'seem', 'absolut', 'normal', 'weird']]\n",
      "[0, 'GoatBorkFright', ['recent', 'take', '100mg', '50mg', 'first', 'week', 'littl', 'ago', 'much', 'alreadi', 'like', 'feel', 'doom', 'life', 'feel', 'sens', 'well', 'put', 'back', 'check', 'still', 'definit', 'hopeless', 'subsid', 'quickli', 'put', 'perspect', 'say', 'longer', 'depress', 'futur', 'hope', 'feel', 'control', 'anyon', 'els', 'taken', 'anti', 'depress', 'felt', 'effect', 'within', 'also', 'taken', 'differ', 'anti', 'differ', 'time', 'throughout', 'last', 'felt', 'sens', 'relief', 'ever']]\n",
      "[1, 'throwaway123094', ['ago', 'left', 'dust', 'act', 'like', 'exist', 'school', 'noth', 'tri', 'reach', 'felt', 'realli', 'like', 'suicid', 'noth', 'recent', 'got', 'one', 'tell', 'stop', 'talk', 'said', 'bring', 'much', 'want', 'deal', 'time', 'understand', 'want', 'talk', 'know', 'rest', 'probabl', 'reason', 'matter', 'thing', 'come', 'rant', 'listen', 'tri', 'help', 'matter', 'even', 'deal', 'busi', 'hell', 'need', 'help', 'say', 'everyth', 'feel', 'aw', 'last', 'time', 'let', 'someon', 'use', 'left', 'dust', 'old', 'boy', 'toy', 'came', 'run', 'back', 'give', 'feel', 'like', 'bring', 'anyth', 'tabl', 'like', 'wast', 'air', 'go', 'keep', 'live', 'feel', 'like', 'open', 'anyon', 'easier']]\n",
      "[0, 'such_a_wonder', ['look', 'advic', 'anyon', 'relat', 'experienc', 'anyon', 'offer', 'path', 'take', 'current', 'much', 'rough', 'childhood', 'grow', 'abus', 'physic', 'emot', 'mother', 'particular', 'would', 'activ', 'harm', 'realiz', 'make', 'ask', 'forgiv', 'repeat', 'least', 'father', 'howev', 'least', 'young', 'circl', 'would', 'make', 'slightli', 'better', 'ye', 'earli', 'life', 'rather', 'unfortun', 'around', 'lost', 'forc', 'move', 'away', 'basic', 'cut', 'like', 'ever', 'sinc', 'never', 'truli', 'connect', 'anyon', 'even', 'famili', 'sinc', 'complet', 'financi', 'independ', 'make', 'good', 'amount', 'money', 'work', 'enjoy', 'live', 'independ', 'past', 'feel', 'like', 'void', 'soul', 'money', 'materi', 'know', 'short', 'forget', 'would', 'like', 'meaning', 'come', 'solut', 'everi', 'time', 'get', 'know', 'someon', 'help', 'think', 'act', 'friendli', 'toward', 'casual', 'go', 'occasion', 'never', 'due', 'massiv', 'fear', 'depress', 'like', 'get', 'wors', 'end', 'sight', 'suicid', 'contribut', 'world', 'although', 'like', 'miss', 'import', 'aspect', 'life', 'want', 'fix', 'thank', 'read', 'origin', 'would', 'like', 'hear', 'advic', 'whether', 'repli', 'privat', 'messag']]\n",
      "[0, 'nirvanabananas', ['first', 'time', 'post', 'also', 'phone', 'spell', 'grammar', 'new', 'first', 'guess', 'two', 'would', 'like', 'think', 'easi', 'go', 'type', 'well', 'got', 'four', 'half', 'ago', 'best', 'friend', 'guy', 'femal', 'realli', 'like', 'much', 'realli', 'still', 'quit', 'raw', 'emot', 'know', 'trigger', 'anyway', 'went', 'polic', 'went', 'court', 'got', 'let', 'away', 'found', 'innoc', 'live', 'great', 'look', 'statist', 'still', 'cannot', 'get', 'head', 'round', 'stop', 'angri', 'viciou', 'cycl', 'act', 'angri', 'bitter', 'guard', 'hurt', 'push', 'away', 'anger', 'fade', 'anyon', 'advic', 'help', 'mayb', 'stop', 'shut', 'everyon', 'big', 'ask', 'realli', 'mean', 'told', 'hardli', 'anyth', 'huge', 'question', 'tri', 'medit', 'realli', 'pretti', 'much', 'open', 'talk', 'peopl', 'advic', 'give', 'thank', 'read', 'sorri', 'hard', 'follow', 'nervou']]\n",
      "[0, 'obveousthrowaway', ['mayb', 'work', 'best', 'realli', 'need', 'stop', 'write', 'wow', 'deepli', 'busi', 'mani', 'edg', 'horribl', 'pleas', 'take', 'anoth', 'decis', 'afford', 'play', 'safe', 'realli', 'harm', 'understand', 'exactli', 'think', 'honest', 'even', 'though', 'would', 'time', 'week', 'crazi', 'crazi', 'busi', 'better', 'go', 'work', 'get', 'job', 'done', 'instead', 'date', 'silli', 'first', 'miss', 'old', 'close', 'dan', 'sure', 'way', 'dan', 'fact', 'also', 'way', 'low', 'guess', 'common', 'knowledg', 'moment', 'leaf', 'away', 'dark', 'cold', 'night', 'feel', 'need', 'somebodi', 'like', 'feel', 'yearn', 'taken', 'care', 'much', 'think', 'well', 'everyth', 'float', 'stream', 'thought', 'float', 'thought', 'normal', 'miser', 'still', 'way', 'feel', 'like', 'final', 'stand', 'enough', 'tri', 'get', 'feel', 'back', 'like', 'everyth', 'simpli', 'relax', 'cold', 'concret', 'dark', 'grey', 'cold', 'blend', 'warm', 'industri', 'giant', 'light', 'oven', 'care', 'someth', 'healthi', 'clean', 'yet', 'complex', 'deep', 'high', 'top', 'floor', 'gaze', 'distanc', 'blue', 'fade', 'sky', 'seep', 'reflect', 'glass', 'oven', 'reflect', 'glass', 'reflect', 'design', 'knit', 'color', 'blue', 'grey', 'green', 'color', 'larg', 'knit', 'gaze', 'effect', 'part', 'feel', 'new', 'place', 'civil', 'kind', 'bore', 'dream', 'guess', 'modern', 'adult', 'grown', 'bit', 'still', 'babi', 'much', 'knew', 'babi', 'nightmar', 'would', 'least', 'give', 'grown', 'add', 'god', 'wish', 'feel', 'easili', 'attach', 'easili', 'attach', 'get', 'know', 'mayb', 'mess', 'mayb', 'see', 'deep', 'despair', 'small', 'inner', 'tini', 'enough', 'enough', 'know', 'eaten', 'enough', 'feel', 'full', 'although', 'actual', 'continu', 'eat', 'well', 'least', 'know', 'work', 'enough', 'task', 'finish', 'exhaust', 'enough', 'sometim', 'peopl', 'never', 'enough', 'alway', 'demand', 'say', 'contribut', 'inadequ', 'weak', 'peopl', 'like', 'white', 'black', 'whole', 'hole', 'need', 'someth', 'made', 'need', 'greater', 'imagin', 'magic', 'evil', 'someon', 'els']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Extracting\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    createTrainingDataset()\n",
    "    createTestingDataset()\n",
    "    #createValidationDataset()\n",
    "    #trainingModel()\n",
    "    #bestTopicModel()\n",
    "    # SVM()\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count:  3352\n",
      "0 'caus',\n",
      "1 'cover',\n",
      "2 'everyth']\n",
      "3 'get',\n",
      "4 'go',\n",
      "5 'help',\n",
      "6 'hospit',\n",
      "7 'insur',\n",
      "8 'know',\n",
      "9 'never',\n",
      "10 'pay',\n",
      "5697\n",
      "450\n",
      "Word 1 (\"'get',\") appears 1 time.\n",
      "Topic: 0 \n",
      "Words: 0.018*\"'time',\" + 0.015*\"'know',\" + 0.014*\"'one',\" + 0.013*\"'even',\" + 0.013*\"'would',\" + 0.013*\"'life',\" + 0.013*\"'realli',\" + 0.012*\"'tri',\" + 0.012*\"'peopl',\" + 0.009*\"'think',\"\n",
      "Topic: 1 \n",
      "Words: 0.028*\"'get',\" + 0.025*\"'want',\" + 0.018*\"'know',\" + 0.016*\"'depress',\" + 0.014*\"'peopl',\" + 0.013*\"'time',\" + 0.013*\"'think',\" + 0.012*\"'life',\" + 0.011*\"'day',\" + 0.011*\"'work',\"\n",
      "Total Count:  4108\n",
      "0 'dose',\n",
      "1 'littl']\n",
      "2 'look',\n",
      "3 'much',\n",
      "4 'see',\n",
      "5 'time',\n",
      "6 ['first',\n",
      "7 'affect',\n",
      "8 'almost',\n",
      "9 'alon',\n",
      "10 'along',\n",
      "5920\n",
      "450\n",
      "Word 0 (\"'look',\") appears 1 time.\n",
      "Word 1 (\"'much',\") appears 3 time.\n",
      "Word 2 (\"'see',\") appears 2 time.\n",
      "Word 3 (\"'time',\") appears 3 time.\n",
      "Word 4 (\"'almost',\") appears 1 time.\n",
      "Word 5 (\"'alon',\") appears 1 time.\n",
      "Word 9 (\"'basic',\") appears 1 time.\n",
      "Word 10 (\"'broke',\") appears 1 time.\n",
      "Word 15 (\"'depress',\") appears 1 time.\n",
      "Word 16 (\"'due',\") appears 1 time.\n",
      "Word 18 (\"'ever',\") appears 2 time.\n",
      "Word 19 (\"'famili',\") appears 3 time.\n",
      "Word 20 (\"'get',\") appears 3 time.\n",
      "Word 22 (\"'help',\") appears 1 time.\n",
      "Word 23 (\"'ill',\") appears 2 time.\n",
      "Word 26 (\"'life',\") appears 2 time.\n",
      "Word 28 (\"'love',\") appears 1 time.\n",
      "Word 29 (\"'mental',\") appears 2 time.\n",
      "Word 31 (\"'money',\") appears 1 time.\n",
      "Word 32 (\"'move',\") appears 1 time.\n",
      "Word 33 (\"'need',\") appears 1 time.\n",
      "Word 34 (\"'never',\") appears 2 time.\n",
      "Word 35 (\"'old',\") appears 1 time.\n",
      "Word 42 (\"'sinc',\") appears 2 time.\n",
      "Word 46 (\"'talk',\") appears 1 time.\n",
      "Word 48 (\"'two',\") appears 1 time.\n",
      "Word 49 (\"'want',\") appears 2 time.\n",
      "Word 51 (\"'wors',\") appears 1 time.\n",
      "Word 57 (\"'deal',\") appears 1 time.\n",
      "Word 65 (\"'problem',\") appears 1 time.\n",
      "Word 70 (\"'suicid',\") appears 2 time.\n",
      "Word 72 (\"'way',\") appears 3 time.\n",
      "Word 74 (\"'even',\") appears 2 time.\n",
      "Word 78 (\"'tri',\") appears 2 time.\n",
      "Word 80 (\"'write',\") appears 2 time.\n",
      "Word 81 (\"'absolut',\") appears 1 time.\n",
      "Word 91 (\"'would',\") appears 1 time.\n",
      "Word 92 (\"'afraid',\") appears 2 time.\n",
      "Word 93 (\"'away',\") appears 1 time.\n",
      "Word 96 (\"'could',\") appears 1 time.\n",
      "Word 97 (\"'cri',\") appears 1 time.\n",
      "Word 101 (\"'give',\") appears 1 time.\n",
      "Word 103 (\"'got',\") appears 2 time.\n",
      "Word 104 (\"'hard',\") appears 1 time.\n",
      "Word 106 (\"'job',\") appears 1 time.\n",
      "Word 109 (\"'mayb',\") appears 1 time.\n",
      "Word 116 (\"'actual',\") appears 1 time.\n",
      "Word 119 (\"'anyon',\") appears 1 time.\n",
      "Word 123 (\"'everi',\") appears 1 time.\n",
      "Word 124 (\"'fact',\") appears 1 time.\n",
      "Word 126 (\"'know',\") appears 9 time.\n",
      "Word 129 (\"'quit',\") appears 2 time.\n",
      "Word 130 (\"'real',\") appears 1 time.\n",
      "Word 131 (\"'say',\") appears 2 time.\n",
      "Word 133 (\"'stop',\") appears 2 time.\n",
      "Word 136 (\"'accept',\") appears 1 time.\n",
      "Word 142 (\"'handl',\") appears 1 time.\n",
      "Word 146 (\"'one',\") appears 4 time.\n",
      "Word 147 (\"'past',\") appears 1 time.\n",
      "Word 150 (\"'still',\") appears 1 time.\n",
      "Word 153 (\"'today',\") appears 1 time.\n",
      "Word 156 (\"'everyon',\") appears 1 time.\n",
      "Word 157 (\"'everyth',\") appears 1 time.\n",
      "Word 168 (\"'home',\") appears 1 time.\n",
      "Word 173 (\"'mani',\") appears 1 time.\n",
      "Word 175 (\"'person',\") appears 1 time.\n",
      "Word 179 (\"'someon',\") appears 1 time.\n",
      "Word 185 (\"'watch',\") appears 2 time.\n",
      "Word 186 (\"'alway',\") appears 1 time.\n",
      "Word 190 (\"'possibl',\") appears 1 time.\n",
      "Word 192 (\"'use',\") appears 1 time.\n",
      "Word 193 (\"'well',\") appears 1 time.\n",
      "Word 196 (\"'day',\") appears 1 time.\n",
      "Word 197 (\"'night',\") appears 1 time.\n",
      "Word 200 (\"'thing',\") appears 2 time.\n",
      "Word 210 (\"'gener',\") appears 1 time.\n",
      "Word 213 (\"'read',\") appears 1 time.\n",
      "Word 217 (\"'first',\") appears 2 time.\n",
      "Word 218 (\"'happi',\") appears 1 time.\n",
      "Word 220 (\"'right',\") appears 1 time.\n",
      "Word 237 (\"'fast',\") appears 1 time.\n",
      "Word 238 (\"'felt',\") appears 1 time.\n",
      "Word 245 (\"'liter',\") appears 1 time.\n",
      "Word 247 (\"'met',\") appears 1 time.\n",
      "Word 250 (\"'three',\") appears 1 time.\n",
      "Word 275 (\"'year',\") appears 2 time.\n",
      "Word 280 (\"'come',\") appears 1 time.\n",
      "Word 288 (\"'last',\") appears 1 time.\n",
      "Word 289 (\"'later',\") appears 1 time.\n",
      "Word 299 (\"'thought',\") appears 1 time.\n",
      "Word 301 (\"'went',\") appears 1 time.\n",
      "Word 307 (\"'may',\") appears 1 time.\n",
      "Word 311 (\"'said',\") appears 3 time.\n",
      "Word 314 (\"'differ',\") appears 1 time.\n",
      "Word 315 (\"'drink',\") appears 4 time.\n",
      "Word 320 (\"'play',\") appears 1 time.\n",
      "Word 324 (\"'whole',\") appears 1 time.\n",
      "Word 334 (\"'guess',\") appears 1 time.\n",
      "Word 337 (\"'kind',\") appears 1 time.\n",
      "Word 343 (\"'abus',\") appears 1 time.\n",
      "Word 344 (\"'apart',\") appears 1 time.\n",
      "Word 345 (\"'came',\") appears 1 time.\n",
      "Word 352 (\"'father',\") appears 1 time.\n",
      "Word 353 (\"'found',\") appears 2 time.\n",
      "Word 356 (\"'knew',\") appears 2 time.\n",
      "Word 360 (\"'mother',\") appears 3 time.\n",
      "Word 361 (\"'pretti',\") appears 1 time.\n",
      "Word 362 (\"'rememb',\") appears 1 time.\n",
      "Word 365 (\"'sister',\") appears 4 time.\n",
      "Word 369 (\"'told',\") appears 2 time.\n",
      "Word 370 (\"'took',\") appears 1 time.\n",
      "Word 391 (\"'half',\") appears 1 time.\n",
      "Word 396 (\"'miss',\") appears 1 time.\n",
      "Word 423 (\"'room',\") appears 1 time.\n",
      "Word 437 (\"'pay',\") appears 1 time.\n",
      "Word 440 (\"'seen',\") appears 1 time.\n",
      "[(0, 0.5574666), (1, 0.44253334)]\n"
     ]
    }
   ],
   "source": [
    "trainingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  997\n",
      "two usual depress therapi quit expens though wonder anyon provid good insight like know therapist like gener idea also use impact peopl wonder treatment right thank\n",
      "assum ask hate probabl known mani\n",
      "well put feel given life without choic never truli meant born world drag age young even think sort way life preciou give still young true genuin mental emot somewhat physic disabl old mother left anyon lean ye step father half sister mention rest famili give support take much intern pain given govern money known social secur disabl incom young naiv sat told realli cours abl help social secur took told lack date back sent profession help group peopl similar sent mani time age old old time due sent place rest alway suicid patient still way younger major depress high anxieti panic autist spectrum disord function limb weak suicid black cognit suffer cancer babi gone chemotherapi make huge toll affect life drastic graduat high school diploma certif complet get colleg tri go ged score averag took 200 money well social secur countless pay 000 even struggl tri find job pay done gotten interview major depress tri file appeal back learn back want abl support properli think make waiver money declin waiver yet think work full time day week job suffer lot mayb day week work day know disabl even find job young person deal realli especi mental hard young peopl find job even harder peopl like get job left noth truli want life end feel worthless dead beat societi helpless nobodi someon valu life wast life blood mockeri human race deal struggl much money come feel take life suffer better end life easier live life everyday pain miser past eat littl dinner depress head dark neg lost appetit due wast away want noth life end feel peac mother one truli known gone without tri kill countless time younger get 100 success rate due fail dad stop suicid self harm done burn skin overdos pain tri tri drink cut wrist swallow foreign tri jump want die\n",
      "dont know miss day work time drink think wors least time better live citi almost weekend without talk singl person reason interact peopl week job satisfact enjoy feel empti insid imposs fill feel broken unfix dark see light sometim think want loneli might biggest culprit unabl meaning friendship current state self bare look mirror mani unabl commun reject never chanc accept make logic know better cant bring realli know post guess thought would good least attempt reach make rash decis\n",
      "hey get good respons thought tri dont realli know talk alway felt better talk peopl feel like peopl know theyr sick tire littl talk sick tire everyth dont actual know that though guess ill tell pretti bad good one theyr think someth depart make degre worthless even though first year five year degre first semest cant seem make like one bad one talk good peopl talk ration dont much doesnt realli work quiet let talk want talk cant think stuff say that sad depress sure ever saw told feel guilti talk dont want either tell love one well love love one romant amaz great amaz person come mind think someon spend rest life that gotten point obsess rememb noth actual feel right girl practic sens know amaz wish love friend love le fact lot dont particularli like love lot like togeth togeth still wish though there also lie guess made believ thought marri believ break mess wish could help inabl commit someon need take care lot lost feel cant much without still though time time strong girl delusion get togeth deal good person there also univers yeah guess wish could leav graduat back job back famili everyth tie dont want think famili two separ left famili lot way good bad like everyth theyr absolut theyr absolut bad absolut good provid beyond need practic princip caviti heart forc via give mani like didnt matter much ye tri talk understand human feel love act way wish could think outsid box career job money would rather clean food shelter happi rich cold clinic manner suggest worst part honest good think help give realiz gove fit 1800 old fashion old peopl old cultur cant realli blame guess that pretti much tri kill thank appar drug tri overdos cant kill wont post name someon make mistak yeah start le le thank die day wish could know sort happi could relationship like wish could attend univers abil studi could someth better vocat job option due class countri famili yeah that pretti much day think smoke cigarett go increas rate wish could tell two dont talk around friendship want last turn artifici think good theyr good peopl sorri took long\n",
      "Started Training\n",
      "Found the best model\n",
      "Best Model's Params:  {'learning_decay': 0.7, 'n_components': 2}\n",
      "Best Log Likelihood Score:  -77766.76897552879\n",
      "Model Perplexity:  478.1329118477133\n"
     ]
    }
   ],
   "source": [
    "bestTopicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  3352\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  1801\n",
      "['financi situat caus problem get help never treatment hospit go pay seem possibl insur know cover everyth'\n",
      " 'new may seen previou question although bit differ normal shi person social anxieti bad find unabl mani feel like could achiev much certain hold back whenev around peopl even close famili find hard talk without shake particular aw head move side side gotten much wors late turn side anoth person like head unabl move twitch embarrass even class use answer alway upset thing realli hold back mani anoth thing alway awkward eye contact long time alway end cough hair even feel nervou around peopl feel peopl even feel confid even anyon els get weird head could never find anyth anyon could tri greatli'\n",
      " 'work list believ stabl true life stay matter someth like talent music learn someth new realli fast realli import apart clean love stuff like music ador musician person make good first impress meet new peopl believ god believ god hope would help identifi respons come work peopl count much differ stuff hope help littl know life never chang'\n",
      " ...\n",
      " 'met girl year ago amaz bodi funni awesom sex multipl time cours happi idea found crap stay far away ran bloodi nose black eye cri say hit rage place broke nose ever fight worth assault charg commun servic later find actual guy see behind back teeth get away told never see depress anxieti long met took hard one person ever fought away tri talk time away week ago street lip told one almost took dig around found true get six jail talk steadili sinc date tomorrow know whether trust realli like want know anoth big lie bad come way final could big time would like advic fresh tri make work updat show date stop talk think go work thank advic though much'\n",
      " 'think peopl depress vice' 'think that']\n",
      "(3352, 2035)\n",
      "479\n",
      "(3352, 2035)\n",
      "(3352, 2035)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "3352\n",
      "['0' '0' '1' ... '0' '0' '0']\n",
      "['1' '0' '0' ... '1' '1' '1']\n",
      "0.6781888997078871\n",
      "[[2532  969]\n",
      " [ 353  254]]\n",
      "[2532  969  353  254]\n"
     ]
    }
   ],
   "source": [
    "#sgd\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  3352\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  1801\n",
      "['financi situat caus problem get help never treatment hospit go pay seem possibl insur know cover everyth'\n",
      " 'new may seen previou question although bit differ normal shi person social anxieti bad find unabl mani feel like could achiev much certain hold back whenev around peopl even close famili find hard talk without shake particular aw head move side side gotten much wors late turn side anoth person like head unabl move twitch embarrass even class use answer alway upset thing realli hold back mani anoth thing alway awkward eye contact long time alway end cough hair even feel nervou around peopl feel peopl even feel confid even anyon els get weird head could never find anyth anyon could tri greatli'\n",
      " 'work list believ stabl true life stay matter someth like talent music learn someth new realli fast realli import apart clean love stuff like music ador musician person make good first impress meet new peopl believ god believ god hope would help identifi respons come work peopl count much differ stuff hope help littl know life never chang'\n",
      " ...\n",
      " 'met girl year ago amaz bodi funni awesom sex multipl time cours happi idea found crap stay far away ran bloodi nose black eye cri say hit rage place broke nose ever fight worth assault charg commun servic later find actual guy see behind back teeth get away told never see depress anxieti long met took hard one person ever fought away tri talk time away week ago street lip told one almost took dig around found true get six jail talk steadili sinc date tomorrow know whether trust realli like want know anoth big lie bad come way final could big time would like advic fresh tri make work updat show date stop talk think go work thank advic though much'\n",
      " 'think peopl depress vice' 'think that']\n",
      "(3352, 2035)\n",
      "479\n",
      "(3352, 2035)\n",
      "(3352, 2035)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "3352\n",
      "['0' '0' '0' ... '0' '0' '0']\n",
      "['1' '0' '0' ... '1' '1' '1']\n",
      "0.7307692307692307\n",
      "[[2813  688]\n",
      " [ 418  189]]\n",
      "[2813  688  418  189]\n"
     ]
    }
   ],
   "source": [
    "#multinomialNB\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  3352\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  1801\n",
      "['financi situat caus problem get help never treatment hospit go pay seem possibl insur know cover everyth'\n",
      " 'new may seen previou question although bit differ normal shi person social anxieti bad find unabl mani feel like could achiev much certain hold back whenev around peopl even close famili find hard talk without shake particular aw head move side side gotten much wors late turn side anoth person like head unabl move twitch embarrass even class use answer alway upset thing realli hold back mani anoth thing alway awkward eye contact long time alway end cough hair even feel nervou around peopl feel peopl even feel confid even anyon els get weird head could never find anyth anyon could tri greatli'\n",
      " 'work list believ stabl true life stay matter someth like talent music learn someth new realli fast realli import apart clean love stuff like music ador musician person make good first impress meet new peopl believ god believ god hope would help identifi respons come work peopl count much differ stuff hope help littl know life never chang'\n",
      " ...\n",
      " 'met girl year ago amaz bodi funni awesom sex multipl time cours happi idea found crap stay far away ran bloodi nose black eye cri say hit rage place broke nose ever fight worth assault charg commun servic later find actual guy see behind back teeth get away told never see depress anxieti long met took hard one person ever fought away tri talk time away week ago street lip told one almost took dig around found true get six jail talk steadili sinc date tomorrow know whether trust realli like want know anoth big lie bad come way final could big time would like advic fresh tri make work updat show date stop talk think go work thank advic though much'\n",
      " 'think peopl depress vice' 'think that']\n",
      "(3352, 2035)\n",
      "479\n",
      "(3352, 2035)\n",
      "(3352, 2035)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "3352\n",
      "['0' '0' '1' ... '0' '0' '0']\n",
      "['1' '0' '0' ... '1' '1' '1']\n",
      "0.638510223953262\n",
      "[[2330 1171]\n",
      " [ 314  293]]\n",
      "[2330 1171  314  293]\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Doc:  3352\n",
      "Suicidal Count:  1551\n",
      "Non-Suicidal Count:  1801\n",
      "['financi situat caus problem get help never treatment hospit go pay seem possibl insur know cover everyth'\n",
      " 'new may seen previou question although bit differ normal shi person social anxieti bad find unabl mani feel like could achiev much certain hold back whenev around peopl even close famili find hard talk without shake particular aw head move side side gotten much wors late turn side anoth person like head unabl move twitch embarrass even class use answer alway upset thing realli hold back mani anoth thing alway awkward eye contact long time alway end cough hair even feel nervou around peopl feel peopl even feel confid even anyon els get weird head could never find anyth anyon could tri greatli'\n",
      " 'work list believ stabl true life stay matter someth like talent music learn someth new realli fast realli import apart clean love stuff like music ador musician person make good first impress meet new peopl believ god believ god hope would help identifi respons come work peopl count much differ stuff hope help littl know life never chang'\n",
      " ...\n",
      " 'met girl year ago amaz bodi funni awesom sex multipl time cours happi idea found crap stay far away ran bloodi nose black eye cri say hit rage place broke nose ever fight worth assault charg commun servic later find actual guy see behind back teeth get away told never see depress anxieti long met took hard one person ever fought away tri talk time away week ago street lip told one almost took dig around found true get six jail talk steadili sinc date tomorrow know whether trust realli like want know anoth big lie bad come way final could big time would like advic fresh tri make work updat show date stop talk think go work thank advic though much'\n",
      " 'think peopl depress vice' 'think that']\n",
      "(3352, 2035)\n",
      "479\n",
      "(3352, 2035)\n",
      "(3352, 2035)\n",
      "Started Training...\n",
      "Done Training...\n",
      "Started Testing...\n",
      "Total Doc:  4108\n",
      "Suicidal Count:  607\n",
      "Non-Suicidal Count:  3501\n",
      "3352\n",
      "['0' '0' '0' ... '0' '0' '0']\n",
      "['1' '0' '0' ... '1' '1' '1']\n",
      "0.7626582278481012\n",
      "[[2969  532]\n",
      " [ 443  164]]\n",
      "[2969  532  443  164]\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
