{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from os import path\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldsForAuthor = [\"author\", \"created_utc\"]\n",
    "fields = [\"author\", \"brand_safe\", \"contest_mode\", \"created_utc\", \"full_link\", \"id\",\n",
    "              \"is_self\", \"num_comments\", \"over_18\", \"retrieved_on\", \"score\", \"selftext\",\n",
    "              \"subreddit\", \"subreddit_id\", \"title\"]\n",
    "fieldsForCommonAuthor = [\"author\", \"generalIssues_created_utc\", \"suicideWatch_created_utc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCloud(filename):\n",
    "    print(\"Creating Word Cloud for Mental Health\")\n",
    "    mentalhealth = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "    mentalhealthImage = np.array(Image.open(\"mentalhealth.png\"))\n",
    "    stopwords = set(wordcloud.STOPWORDS)\n",
    "    stopwords.add(\"english\")  # to get rid of the most common words like \"the\", \"it\", \"of\" etc\n",
    "    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000, mask=mentalhealthImage, max_font_size=40,\n",
    "                             stopwords=stopwords, random_state=42)\n",
    "    wc.generate(mentalhealth)\n",
    "    print(\"Done generating words!\")\n",
    "    mental_health_colors = wordcloud.ImageColorGenerator(mentalhealthImage)\n",
    "    plt.imshow(wc.recolor(color_func=mental_health_colors), interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    print(\"Created Word Cloud for Mental Health\")\n",
    "    # plt.figure()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCloud2(filename):\n",
    "    print(\"Creating Word Cloud for Suicide Watch\")\n",
    "    suicidewatch = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "    suicidewatchImage = np.array(Image.open( \"suicide.png\"))\n",
    "    stopwords = set(wordcloud.STOPWORDS)\n",
    "    stopwords.add(\"english\")  # to get rid of the most common words like \"the\", \"it\", \"of\" etc\n",
    "    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000, mask=suicidewatchImage, max_font_size=40,\n",
    "                             stopwords=stopwords, random_state=42)\n",
    "    wc.generate(suicidewatch)\n",
    "    suicide_colors = wordcloud.ImageColorGenerator(suicidewatchImage)\n",
    "    plt.imshow(wc.recolor(color_func=suicide_colors), interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    print(\"Created Word Cloud for Suicide Watch\")\n",
    "    # plt.figure()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMentalHealthCSV(start, end):\n",
    "    with open('generalIssuesTS2.csv', mode='a',encoding=\"utf-8\") as fileObject:\n",
    "        csvWriter = csv.writer(fileObject, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow(fields)\n",
    "        for delta in range(1, 61):\n",
    "            start += datetime.timedelta(days=1)\n",
    "            end += datetime.timedelta(days=1)\n",
    "            epoch1 = int(time.mktime(start.timetuple()))\n",
    "            epoch2 = int(time.mktime(end.timetuple()))\n",
    "            mentalHealth = \"https://api.pushshift.io/reddit/search/submission/?after={0}&before={1}&size={2}&subreddit={3}\".format(\n",
    "                epoch1, epoch2, '1000', 'mentalhealth')\n",
    "            generalIssues = \"https://api.pushshift.io/reddit/search/submission/?after={0}&before={1}&size={2}&subreddit={3}\".format(\n",
    "                epoch1, epoch2, '1000',\n",
    "                'mentalhealth,depression,traumatoolbox,bipolarreddit,BPD,ptsd,psychoticreddit,EatingDisorders,StopSelfHarm,survivorsofabuse,rapecounseling,hardshipmates,panicparty,socialanxiety')\n",
    "            suicideWatch = \"https://api.pushshift.io/reddit/search/submission/?after={0}&before={1}&size={2}&subreddit={3}\".format(\n",
    "                epoch1, epoch2, '1000', 'suicidewatch')\n",
    "            url = \"https://api.pushshift.io/reddit/search/submission/?after=1489208400&before=1502424000&size=40000&subreddit=mentalhealth\"\n",
    "\n",
    "            data = requests.get(generalIssues)\n",
    "            data = data.json()\n",
    "            count = 0\n",
    "            for singlePost in data[\"data\"]:\n",
    "                row = []\n",
    "                for field in fields:\n",
    "                    row.append(singlePost.get(field, None))\n",
    "                count += 1\n",
    "                csvWriter.writerow(row)\n",
    "            print(start, end, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAuthorsWithTimestamp(fromFile, toFile):\n",
    "    print(\"Extrating Authors from: \", fromFile, \"to: \", toFile)\n",
    "    tempSet = set()\n",
    "    with open(fromFile, mode='r',encoding=\"utf-8\") as fileReader:\n",
    "        fileReader.readline()\n",
    "        csvReader = csv.reader(fileReader, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                if row[0] != \"[deleted]\":\n",
    "                    tempSet.add(row[0])\n",
    "    with open(toFile, mode='w',encoding=\"utf-8\") as fileWriter:\n",
    "        csvWriter = csv.writer(fileWriter, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow([fieldsForAuthor[0]])\n",
    "        for elem in tempSet:\n",
    "            csvWriter.writerow([elem])\n",
    "    print(\"Done Extracting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMHandSWcommonAuthors(generalIssuesFilename, suicideWatchFilename, commonAuthorsFilename):\n",
    "    print(\"Extracting Common Authors between: \", generalIssuesFilename, \"and: \", suicideWatchFilename)\n",
    "    swSet = set()\n",
    "    with open(suicideWatchFilename, mode='r') as swReader:\n",
    "        swReader.readline()\n",
    "        csvSWReader = csv.reader(swReader, delimiter=',')\n",
    "        for row in csvSWReader:\n",
    "            if len(row)>=1:\n",
    "                swSet.add(row[0])\n",
    "\n",
    "    giSet = set()\n",
    "    with open(generalIssuesFilename, mode='r') as giReader:\n",
    "        giReader.readline()\n",
    "        csvGIReader = csv.reader(giReader, delimiter=',')\n",
    "        for row in csvGIReader:\n",
    "            if len(row)>=1:\n",
    "                giSet.add(row[0])\n",
    "\n",
    "    common = swSet & giSet\n",
    "\n",
    "    with open(commonAuthorsFilename, mode='w') as commonWriter:\n",
    "        csvWriter = csv.writer(commonWriter, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow([fieldsForCommonAuthor[0]])\n",
    "        for elem in common:\n",
    "            csvWriter.writerow([elem])\n",
    "    print(\"Done Extracting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAllDataForCommonAuthors(postsFilename, commonAuthorsFilename, commonPostsFilename):\n",
    "    commonAuthors = set()\n",
    "    with open(commonAuthorsFilename, mode='r',encoding='utf-8') as commonReader:\n",
    "        commonReader.readline()\n",
    "        csvReader = csv.reader(commonReader, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            if len(row)>=1:\n",
    "                commonAuthors.add(row[0])\n",
    "\n",
    "    with open(commonPostsFilename, mode='w',encoding='utf-8') as commonWriter:\n",
    "        csvWriter = csv.writer(commonWriter, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvWriter.writerow(fields)\n",
    "        with open(postsFilename, mode='r',encoding='utf-8') as commonReader:\n",
    "            commonReader.readline()\n",
    "            csvReader = csv.reader(commonReader, delimiter=',')\n",
    "            for row in csvReader:\n",
    "                if len(row)>=1:\n",
    "                    if row[0] in commonAuthors:\n",
    "                        csvWriter.writerow(row)\n",
    "\n",
    "    print(len(commonAuthors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-14 2016-08-15 100\n",
      "2016-08-15 2016-08-16 100\n",
      "2016-08-16 2016-08-17 100\n",
      "2016-08-17 2016-08-18 100\n",
      "2016-08-18 2016-08-19 100\n",
      "2016-08-19 2016-08-20 100\n",
      "2016-08-20 2016-08-21 100\n",
      "2016-08-21 2016-08-22 100\n",
      "2016-08-22 2016-08-23 100\n",
      "2016-08-23 2016-08-24 100\n",
      "2016-08-24 2016-08-25 100\n",
      "2016-08-25 2016-08-26 100\n",
      "2016-08-26 2016-08-27 100\n",
      "2016-08-27 2016-08-28 100\n",
      "2016-08-28 2016-08-29 100\n",
      "2016-08-29 2016-08-30 100\n",
      "2016-08-30 2016-08-31 100\n",
      "2016-08-31 2016-09-01 100\n",
      "2016-09-01 2016-09-02 100\n",
      "2016-09-02 2016-09-03 100\n",
      "2016-09-03 2016-09-04 100\n",
      "2016-09-04 2016-09-05 100\n",
      "2016-09-05 2016-09-06 100\n",
      "2016-09-06 2016-09-07 100\n",
      "2016-09-07 2016-09-08 100\n",
      "2016-09-08 2016-09-09 100\n",
      "2016-09-09 2016-09-10 100\n",
      "2016-09-10 2016-09-11 100\n",
      "2016-09-11 2016-09-12 100\n",
      "2016-09-12 2016-09-13 100\n",
      "2016-09-13 2016-09-14 100\n",
      "2016-09-14 2016-09-15 100\n",
      "2016-09-15 2016-09-16 100\n",
      "2016-09-16 2016-09-17 100\n",
      "2016-09-17 2016-09-18 100\n",
      "2016-09-18 2016-09-19 100\n",
      "2016-09-19 2016-09-20 100\n",
      "2016-09-20 2016-09-21 100\n",
      "2016-09-21 2016-09-22 100\n",
      "2016-09-22 2016-09-23 100\n",
      "2016-09-23 2016-09-24 100\n",
      "2016-09-24 2016-09-25 100\n",
      "2016-09-25 2016-09-26 100\n",
      "2016-09-26 2016-09-27 100\n",
      "2016-09-27 2016-09-28 100\n",
      "2016-09-28 2016-09-29 100\n",
      "2016-09-29 2016-09-30 100\n",
      "2016-09-30 2016-10-01 100\n",
      "2016-10-01 2016-10-02 100\n",
      "2016-10-02 2016-10-03 100\n",
      "2016-10-03 2016-10-04 100\n",
      "2016-10-04 2016-10-05 100\n",
      "2016-10-05 2016-10-06 100\n",
      "2016-10-06 2016-10-07 100\n",
      "2016-10-07 2016-10-08 100\n",
      "2016-10-08 2016-10-09 100\n",
      "2016-10-09 2016-10-10 100\n",
      "2016-10-10 2016-10-11 100\n",
      "2016-10-11 2016-10-12 100\n",
      "2016-10-12 2016-10-13 100\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    reddit = praw.Reddit(client_id='efSXI-Jfy0KzYg', client_secret='mcqrfuhJgZjwowavwLkl_uoT3mxPUA',\n",
    "                          user_agent='dataset', username='mohit_mittal', password='major_project')\n",
    "\n",
    "    #wordCloud('mentalHealth.txt')\n",
    "    #wordCloud2('suicidewatch.txt')\n",
    "    mentalHealthSubreddit = reddit.subreddit('generalIssues')\n",
    "    ##TS1\n",
    "    #start = datetime.date(2016, 2, 11)\n",
    "    #end = datetime.date(2016, 2, 12)\n",
    "    #loop from 1 to 61, then append below\n",
    "    #TS1\n",
    "    #start = datetime.date(2016, 4, 13)\n",
    "    #end = datetime.date(2016, 4, 14)\n",
    "    #loop from 1 to 61, then append below\n",
    "    #TS1\n",
    "    #start = datetime.date(2016, 6, 13)\n",
    "    #end = datetime.date(2016, 6, 14)\n",
    "    #loop from 1 to 61, then append below\n",
    "    #TS1\n",
    "    start = datetime.date(2016, 8, 13)\n",
    "    end = datetime.date(2016, 8, 14)\n",
    "    extractMentalHealthCSV(start, end)\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-13 2016-12-14 100\n",
      "2016-12-14 2016-12-15 100\n",
      "2016-12-15 2016-12-16 100\n",
      "2016-12-16 2016-12-17 100\n",
      "2016-12-17 2016-12-18 100\n",
      "2016-12-18 2016-12-19 100\n",
      "2016-12-19 2016-12-20 100\n",
      "2016-12-20 2016-12-21 100\n",
      "2016-12-21 2016-12-22 100\n",
      "2016-12-22 2016-12-23 100\n",
      "2016-12-23 2016-12-24 100\n",
      "2016-12-24 2016-12-25 100\n",
      "2016-12-25 2016-12-26 100\n",
      "2016-12-26 2016-12-27 100\n",
      "2016-12-27 2016-12-28 100\n",
      "2016-12-28 2016-12-29 100\n",
      "2016-12-29 2016-12-30 100\n",
      "2016-12-30 2016-12-31 100\n",
      "2016-12-31 2017-01-01 100\n",
      "2017-01-01 2017-01-02 100\n",
      "2017-01-02 2017-01-03 100\n",
      "2017-01-03 2017-01-04 100\n",
      "2017-01-04 2017-01-05 100\n",
      "2017-01-05 2017-01-06 100\n",
      "2017-01-06 2017-01-07 100\n",
      "2017-01-07 2017-01-08 100\n",
      "2017-01-08 2017-01-09 100\n",
      "2017-01-09 2017-01-10 100\n",
      "2017-01-10 2017-01-11 100\n",
      "2017-01-11 2017-01-12 100\n",
      "2017-01-12 2017-01-13 100\n",
      "2017-01-13 2017-01-14 100\n",
      "2017-01-14 2017-01-15 100\n",
      "2017-01-15 2017-01-16 100\n",
      "2017-01-16 2017-01-17 100\n",
      "2017-01-17 2017-01-18 100\n",
      "2017-01-18 2017-01-19 100\n",
      "2017-01-19 2017-01-20 100\n",
      "2017-01-20 2017-01-21 100\n",
      "2017-01-21 2017-01-22 100\n",
      "2017-01-22 2017-01-23 100\n",
      "2017-01-23 2017-01-24 100\n",
      "2017-01-24 2017-01-25 100\n",
      "2017-01-25 2017-01-26 100\n",
      "2017-01-26 2017-01-27 100\n",
      "2017-01-27 2017-01-28 100\n",
      "2017-01-28 2017-01-29 100\n",
      "2017-01-29 2017-01-30 100\n",
      "2017-01-30 2017-01-31 100\n",
      "2017-01-31 2017-02-01 100\n",
      "2017-02-01 2017-02-02 100\n",
      "2017-02-02 2017-02-03 100\n",
      "2017-02-03 2017-02-04 100\n",
      "2017-02-04 2017-02-05 100\n",
      "2017-02-05 2017-02-06 100\n",
      "2017-02-06 2017-02-07 100\n",
      "2017-02-07 2017-02-08 100\n",
      "2017-02-08 2017-02-09 100\n",
      "2017-02-09 2017-02-10 100\n",
      "2017-02-10 2017-02-11 100\n"
     ]
    }
   ],
   "source": [
    "#TS2\n",
    "reddit = praw.Reddit(client_id='efSXI-Jfy0KzYg', client_secret='mcqrfuhJgZjwowavwLkl_uoT3mxPUA',\n",
    "                          user_agent='dataset', username='mohit_mittal', password='major_project')\n",
    "mentalHealthSubreddit = reddit.subreddit('generalIssues')\n",
    "#start = datetime.date(2016, 10, 12)\n",
    "#end = datetime.date(2016, 10, 13)\n",
    "start = datetime.date(2016, 12, 12)\n",
    "end = datetime.date(2016, 12, 13)\n",
    "extractMentalHealthCSV(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrating Authors from:  suicideWatchTS1.csv to:  suicideWatchAuthorsTS1.csv\n",
      "Done Extracting!\n",
      "Extrating Authors from:  suicideWatchTS2.csv to:  suicideWatchAuthorsTS2.csv\n",
      "Done Extracting!\n"
     ]
    }
   ],
   "source": [
    "#extractAuthorsWithTimestamp('mentalHealthTS1.csv', 'mentalHealthAuthorsTS1.csv')\n",
    "#extractAuthorsWithTimestamp('mentalHealthTS2.csv', 'mentalHealthAuthorsTS2.csv')\n",
    "extractAuthorsWithTimestamp('suicideWatchTS1.csv', 'suicideWatchAuthorsTS1.csv')\n",
    "extractAuthorsWithTimestamp('suicideWatchTS2.csv', 'suicideWatchAuthorsTS2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrating Authors from:  generalIssuesTS1.csv to:  generalIssuesAuthorsTS1.csv\n",
      "Done Extracting!\n",
      "Extrating Authors from:  generalIssuesTS2.csv to:  generalIssuesAuthorsTS2.csv\n",
      "Done Extracting!\n"
     ]
    }
   ],
   "source": [
    "extractAuthorsWithTimestamp('generalIssuesTS1.csv', 'generalIssuesAuthorsTS1.csv')\n",
    "extractAuthorsWithTimestamp('generalIssuesTS2.csv', 'generalIssuesAuthorsTS2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Common Authors between:  generalIssuesAuthorsTS1.csv and:  suicideWatchAuthorsTS1.csv\n",
      "Done Extracting\n",
      "Extracting Common Authors between:  generalIssuesAuthorsTS2.csv and:  suicideWatchAuthorsTS2.csv\n",
      "Done Extracting\n"
     ]
    }
   ],
   "source": [
    "#extractMHandSWcommonAuthors('mentalHealthAuthorsTS1.csv', 'suicideWatchAuthorsTS1.csv', 'mhTS1swTS1Authors.csv')\n",
    "#extractMHandSWcommonAuthors('mentalHealthAuthorsTS2.csv', 'suicideWatchAuthorsTS2.csv', 'mhTS2swTS2Authors.csv')\n",
    "extractMHandSWcommonAuthors('generalIssuesAuthorsTS1.csv', 'suicideWatchAuthorsTS1.csv', 'giTS1swTS1Authors.csv')\n",
    "#extractMHandSWcommonAuthors('generalIssuesAuthorsTS1.csv', 'suicideWatchAuthorsTS2.csv', 'giTS1swTS2Authors.csv')\n",
    "extractMHandSWcommonAuthors('generalIssuesAuthorsTS2.csv', 'suicideWatchAuthorsTS2.csv', 'giTS2swTS2Authors.csv')\n",
    "#extractMHandSWcommonAuthors('generalIssuesAuthorsTS2.csv', 'suicideWatchAuthorsTS1.csv', 'giTS2swTS1Authors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "extractAllDataForCommonAuthors('generalIssuesTS1.csv', 'giTS1swTS1Authors.csv', 'generalIssuesCommonTS1.csv')\n",
    "extractAllDataForCommonAuthors('generalIssuesTS2.csv', 'giTS2swTS2Authors.csv', 'generalIssuesCommonTS2.csv')\n",
    "#extractAllDataForCommonAuthors('generalIssuesTS1.csv', 'giTS1swTS2Authors.csv', 'generalIssuesCommonTS1toTS2.csv')\n",
    "#extractAllDataForCommonAuthors('generalIssuesTS2.csv', 'giTS2swTS1Authors.csv', 'generalIssuesCommonTS2toTS1.csv')\n",
    "#extractAllDataForCommonAuthors('generalIssuesTS1.csv', 'mhTS1swTS1Authors.csv', 'allCommonTS1.csv')\n",
    "#extractAllDataForCommonAuthors('generalIssuesTS2.csv', 'mhTS2swTS2Authors.csv', 'allCommonTS2.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
